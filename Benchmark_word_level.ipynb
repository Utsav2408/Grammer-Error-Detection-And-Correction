{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Benchmark_word_level.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3DMBfFOYRnzv"},"source":["# ENCODER DECODER MODEL FOR WORD LEVEL EMBEDDING"],"id":"3DMBfFOYRnzv"},{"cell_type":"code","metadata":{"id":"a083ffd7"},"source":["## LOADING THE REQUIRED LIBRARIES\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm \n","import tensorflow as tf\n","from  tensorflow.keras.preprocessing.sequence import pad_sequences\n","from  sklearn.model_selection import train_test_split\n","from tqdm import tqdm"],"id":"a083ffd7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7345a1b8"},"source":["## Loading Dataset"],"id":"7345a1b8"},{"cell_type":"code","metadata":{"id":"47TEFxlLGkHY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"47TEFxlLGkHY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f2b8072"},"source":["## LOADING THE PROCESSED DATASET  \n","\n","df= pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/cs2/processed_data.csv\")\n","df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n","df[\"dec_output\"] = df.dec_input"],"id":"4f2b8072","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5682004c"},"source":["## Adding start and end token"],"id":"5682004c"},{"cell_type":"code","metadata":{"id":"9cd63a6e"},"source":["## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n","## <start> AS BEGINING TOKEN\n","## <end>  AS END TOKEN\n","\n","df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n","df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" "],"id":"9cd63a6e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54c6bfe3"},"source":["## Splitting And Sampling around 100k datapoints\n","\n","---\n","##### THE TOTAL DATASET HAS 500K DATAPOINTS WHICH WILL TAKE MUCH HIGHER TRAINING TIME. THEREFORE I AM SAMPLING ONE-FIFTH OF THE TOTAL DATASET\n","\n"],"id":"54c6bfe3"},{"cell_type":"code","metadata":{"id":"T1NgQRsLxM1r"},"source":["df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))"],"id":"T1NgQRsLxM1r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHbUAaYMWGMU"},"source":["## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n","\n","df_train ,df_val = train_test_split(df_sampled,test_size=0.2,random_state = 3, stratify = df_sampled.y )"],"id":"JHbUAaYMWGMU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"261827ba"},"source":["## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n","\n","df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\""],"id":"261827ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3dYYO32O9uf"},"source":["## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n","np.random.seed(5) \n","df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]"],"id":"O3dYYO32O9uf","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b4ffe0f"},"source":["## Tokenization"],"id":"4b4ffe0f"},{"cell_type":"code","metadata":{"id":"6a248032"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"id":"6a248032","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6f538d5"},"source":["## TOKENIZER FOR ENCODER INPUT\n","tk_inp = Tokenizer()\n","tk_inp.fit_on_texts(df_train.enc_input.apply(str))"],"id":"a6f538d5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e953882"},"source":["# TOKENIZER FOR DECODER INPUT\n","tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n","tk_out.fit_on_texts(df_train.dec_input.apply(str))"],"id":"3e953882","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45d861da"},"source":["## DATA PIPELINE"],"id":"45d861da"},{"cell_type":"code","metadata":{"id":"0a905874"},"source":["## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n","\n","class Dataset :\n","    def __init__(self, data , tk_inp ,tk_out, max_len):\n","        ## SETTING THE REQUIRED ATTRIBUTES\n","        self.encoder_inp = data[\"enc_input\"].apply(str).values\n","        self.decoder_inp = data[\"dec_input\"].apply(str).values\n","        self.decoder_out = data[\"dec_output\"].apply(str).values\n","        self.tk_inp = tk_inp\n","        self.tk_out = tk_out\n","        self.max_len = max_len\n","        \n","    def __getitem__(self,i):\n","        # INPUT SEQUENCES\n","        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n","        # DECODER INPUT SEQUENCES \n","        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n","        # DECODER INPUT SEQUENCES\n","        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n","        \n","        # PADDING THE ENCODER INPUT SEQUENCES\n","        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING THE DECODER INPUT SEQUENCES\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING DECODER OUTPUT SEQUENCES\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n","\n","        ##  RETURNING THE ENCODER INPUT , DECODER INPUT , AND DECODER OUTPUT\n","        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n","    \n","    def __len__(self):\n","        # RETURN THE LEN OF INPUT ENDODER\n","        return len(self.encoder_inp)"],"id":"0a905874","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9a336e"},"source":["## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n","\n","class Dataloader(tf.keras.utils.Sequence):\n","    def __init__(self,batch_size,dataset):\n","        # INTIALIZING THE REQUIRED VARIABLES \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.totl_points = self.dataset.encoder_inp.shape[0]\n","        \n","    def __getitem__(self,i):\n","        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n","        start = i * self.batch_size\n","        stop = (i+1)*self.batch_size\n","        \n","        # PLACEHOLDERS FOR BATCHED DATA\n","        batch_enc =[]\n","        batch_dec_input = []\n","        batch_dec_out =[]\n","\n","        for j in range(start,stop): \n","            \n","            a,b,c = self.dataset[j] \n","            batch_enc.append(a[0]) \n","            batch_dec_input.append(b[0])\n","            batch_dec_out.append(c[0]) \n","        \n","        # Conveting list to array   \n","        batch_enc = (np.array(batch_enc)) \n","        batch_dec_input = np.array(batch_dec_input)\n","        batch_dec_out = np.array(batch_dec_out)\n","        \n","        ## RETURNING BATCHED DATA IN REQUIRED FORM\n","        return [batch_enc , batch_dec_input],batch_dec_out\n","    \n","    def __len__(self):\n","        # Returning the number of batches\n","        return int(self.totl_points/self.batch_size)"],"id":"2c9a336e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfEi9fzDR3FD"},"source":["###### NOTE: WE ARE TAKING THE MAXIMUM LENGHT EQUAL TO 35 WHICH IS 99 PERCENTILE OF THE WORD LENGTH DISTRUBUTION"],"id":"QfEi9fzDR3FD"},{"cell_type":"code","metadata":{"id":"86d7d05b"},"source":["# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n","train_dataset = Dataset(df_train,tk_inp,tk_out,35)\n","train_dataloader = Dataloader( batch_size = 512, dataset=train_dataset)\n","\n","# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n","val_dataset = Dataset(df_val , tk_inp,tk_out,35)\n","val_dataloader = Dataloader(batch_size=512 , dataset=val_dataset)"],"id":"86d7d05b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0abfa68"},"source":["## Encoder Decoder Model"],"id":"e0abfa68"},{"cell_type":"code","metadata":{"id":"d06e5d1f"},"source":["## LOADING THE TENSORFLOW LIBRARIES\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model"],"id":"d06e5d1f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXRxqbRYSEv2"},"source":["## DEFINING THE ENCODER LAYER AS A FUNCTION\n","\n","def encoder(input_shape,vocab, emb_output, lstm_units, enc_input):\n","    '''THIS FUNCTION TAKES IN THE SEQUENCES AND RETURNS THE ENCODER OUTPUT'''\n","    ## FIRST LAYER : EMBEDDING LAYER\n","    enc_emb = layers.Embedding(vocab, emb_output,mask_zero = True,input_length=input_shape)(enc_input)\n","    ## SECOND LAYER : LSTM LAYER\n","    enc_lstm , enc_state_h,enc_state_c = layers.LSTM(units= lstm_units,return_sequences=True,return_state=True)(enc_emb)\n","    ## RETURNING THE LSTM OUTPUTS AND STATES\n","    return enc_lstm , enc_state_h,enc_state_c\n","\n","\n","## DEFINING THE DECODER LAYER AS A FUNCTION \n","def decoder(input_shape,vocab, emb_output, lstm_units,enc_states, dec_input):\n","  ## FIRST LAYER : EMBEDDING LAYER\n","  dec_emb = layers.Embedding(vocab, emb_output , mask_zero = True,input_length=input_shape)(dec_input)\n","  ## SECONG LAYER : LSTM LAYER\n","  dec_lstm, dec_state_h,dec_state_c = layers.LSTM(units=lstm_units,return_sequences=True,return_state=True)(dec_emb,initial_state= enc_states)\n","  ## RETURNING THE LSTM OUTPUTS AND STATES\n","  return dec_lstm, dec_state_h,dec_state_c"],"id":"zXRxqbRYSEv2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7884d4e2"},"source":["## DEFINING THE MODEL ARCHITECTURE\n","\n","# INPUT LAYER\n","enc_input = layers.Input(shape=(35))\n","# ENCODER DEFINED FORM FUNCTON ABOVE\n","enc_lstm , enc_state_h,enc_state_c = encoder(35,len(tk_inp.word_index)+1 , 300 ,256, enc_input )\n","\n","\n","# DECODER INPUT LAYER\n","dec_input = layers.Input(shape = (35))\n","# DECODER DEFINEA FROM ABOVE FUNCTION\n","dec_lstm , dec_state_h,dec_state_c = decoder(35,len(tk_out.word_index)+1 , 300 , 256 , [enc_state_h,enc_state_c],dec_input)\n","# DENCSE LAYER CONNECTOD TO DECODER OUTPUT\n","dense = layers.Dense(len(tk_out.word_index)+1,activation=\"softmax\")(dec_lstm)\n","\n","# MODEL DEFINING\n","model  = Model(inputs=[enc_input,dec_input],outputs=dense)\n"],"id":"7884d4e2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpct2PtDjuhm"},"source":["## DEFINING THE CALLBACKS\n","callback =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001)\n","]\n","\n","## STORING THE NUMBER OF STEPS IN ONE EPOCH FOR TRAIN AND VALIDATION DATASET\n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","# COMPILING THE MODEL\n","model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"],"id":"jpct2PtDjuhm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qmpe_rIJREqf"},"source":["## FITTING THE MODEL\n","model.fit(train_dataloader,steps_per_epoch=train_steps,epochs=50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"],"id":"Qmpe_rIJREqf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMAQUT5_9QYg"},"source":["# LOADING THE WEIGHTS FOR BEST MODEL\n","model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")"],"id":"jMAQUT5_9QYg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3967f5b9"},"source":["## THIS FUNCTION IS USED IN THE INFERENCE TIME TO PREDICT THE RESULTS GIVEN THE INPUT TEXT\n","\n","def predict(inp , model):\n","    ##  TAKES INPUT AS TEXT AND THE MODEL\n","\n","    # CONVERT TEXT INPUT TO SEQUENCES \n","    seq = tk_inp.texts_to_sequences([inp])\n","    # PADDING THE SEQUENCE\n","    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n","    ## INITIAL STATES FOR ENCODER\n","    state = [tf.zeros(shape=(1,256)),tf.zeros(shape= (1,256))]\n","\n","    # SEQUENCE TO EMBEDDING\n","    enc_emb  = model.layers[2](seq)\n","    # PASSING EMBBEDDED SEQUENCES TO LSTM LAYER\n","    enc_output,state_h,state_c= model.layers[4](enc_emb,state)\n","\n","    # PLACE HOLDER FOR PREDECTED WORDS\n","    pred = []\n","    # PLACE HOLDER FOR STATES \n","    input_state = [state_h,state_c]\n","    # CURRENT VECTOR TO BE PASSED TO DECODER \n","    current_vec = tf.ones((1,1))\n","    \n","    for i in range(35): # FOR i UP TO 35 (MAX LENGTH)\n","        ## CONVERT THE CURRENT VECTOR SEQUENCE WORD TO EMBEDDINGS\n","        dec_emb  = model.layers[3](current_vec)\n","        ## PASSING EMBEDDED VECTOR TO DECODER LSTM LAYER\n","        dec_output,dec_state_h,dec_state_c = model.layers[5](dec_emb , input_state)\n","        # PASSING DECODER OUTPUT TO DENSE LAYER\n","        dense = model.layers[6](dec_output)\n","\n","        # SELECTING INDEX OF MAXIMUM DENSE OUTPUT AS CURRENT VECTOR\n","        current_vec = np.argmax(dense ,axis = -1)\n","        # UPDATING THE INPUT STATES\n","        input_state = [dec_state_h,dec_state_c]\n","\n","        # APPENDING THE ACTUAL TEXT TO \"pred\" VARIABLE\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","        ## IF THE CURRENT VECTOR IS \"<end>\" BREAK THE LOOP\n","        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n","            break\n","    ## RETURN THE JOINED STRING IN LIST \"pred\"\n","    return \" \".join(pred)"],"id":"3967f5b9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAa6fiPQZuB8"},"source":["## PREDICTIONS ON TEST DATASET"],"id":"JAa6fiPQZuB8"},{"cell_type":"code","metadata":{"id":"xassbnl5Ztnx"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"],"id":"xassbnl5Ztnx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5W5TvCvraLxR"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"],"id":"5W5TvCvraLxR","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7o-BfiMjxR3"},"source":["## Inference Time"],"id":"A7o-BfiMjxR3"},{"cell_type":"code","metadata":{"id":"FPfYEnLyjwwt"},"source":["%%time\n","predict(df_test.enc_input.values[50],model)"],"id":"FPfYEnLyjwwt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFvYYz8zjjRT"},"source":["## BELU SCORE "],"id":"TFvYYz8zjjRT"},{"cell_type":"code","metadata":{"id":"zexRbu3Uugc4"},"source":["import nltk.translate.bleu_score as bleu"],"id":"zexRbu3Uugc4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQRf1BZ9Dl6F"},"source":["# VALIDATION BELU SCORE\n","BLEU_val_emb = []\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = predict(str(i.enc_input),model).split()\n","        act = [str(i.dec_output).split()]\n","        b =bleu.sentence_bleu(act,pred)\n","        BLEU_val_emb.append(b)\n","    except:\n","        continue"],"id":"UQRf1BZ9Dl6F","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37sdVN9L9zIl"},"source":["print(\"Triain BELU Score = \",np.mean(BLEU_val_emb))"],"id":"37sdVN9L9zIl","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIDkEeudrRHo"},"source":["##  Model2 - Pre Trained Word Vector"],"id":"DIDkEeudrRHo"},{"cell_type":"code","metadata":{"id":"245d0f83"},"source":["!wget --header=\"Host: doc-14-58-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://drive.google.com/\" --header=\"Cookie: AUTH_oli2bj78otd6bj690sskhutcvt4jcljv_nonce=a17t06p88k2ng\" --header=\"Connection: keep-alive\" \"https://doc-14-58-docs.googleusercontent.com/docs/securesc/l8ia6k4p0fn1kebbsov8ekehei507757/onk0h25n3rm35gh67s449ql1utt8rkp0/1628269950000/06848720943842814915/12599792132870778014/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download&authuser=0&nonce=a17t06p88k2ng&user=12599792132870778014&hash=uvrme0lsn4ltpk6ivnbe741c9984e4ih\" -c -O 'GoogleNews-vectors-negative300.bin.gz'"],"id":"245d0f83","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODp22qSpap5Y"},"source":["## LOADING THE WORD2VEC MODEL \n","\n","from gensim import models\n","w = models.KeyedVectors.load_word2vec_format(r\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"],"id":"ODp22qSpap5Y","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ym7GTc-DaxRu"},"source":["## FORMING EMBEDDING MATRIX FOR INPUT SENTENCE\n","embedding_matrix_input = np.zeros((len(tk_inp.word_index)+1,300))\n","\n","for word,index in tqdm(tk_inp.word_index.items()) :\n","    try:\n","        vec = w[word]\n","        embedding_matrix_input[index] = vec\n","    except:\n","      \n","        continue"],"id":"ym7GTc-DaxRu","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTFCHb6Pexsk"},"source":["# FORMING EMBEDING MATRIX FOR OUTPUT SENTENCE\n","embedding_matrix_output = np.zeros((len(tk_out.word_index)+1,300))\n","\n","for word,index in tqdm(tk_out.word_index.items()) :\n","    try:\n","        vec = w[word]\n","        embedding_matrix_output[index] = vec\n","    except:\n","   \n","        continue"],"id":"hTFCHb6Pexsk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVtfi_PvQ-FO"},"source":["## MODEL ARCHITECTURE"],"id":"pVtfi_PvQ-FO"},{"cell_type":"code","metadata":{"id":"IqiHNviYsD-A"},"source":["def encoder2(input_shape,vocab, emb_output, lstm_units, enc_input,embedding_matrix):\n","  \n","  enc_emb = layers.Embedding(vocab, emb_output,weights= [embedding_matrix] ,mask_zero = True,input_length=input_shape,trainable=False)(enc_input)\n","  enc_lstm , enc_state_h,enc_state_c = layers.LSTM(units= lstm_units,return_sequences=True,return_state=True)(enc_emb)\n","  return enc_lstm , enc_state_h,enc_state_c\n","\n","def decoder2(input_shape,vocab, emb_output, lstm_units,enc_states, dec_input,embedding_matrix):\n","  \n","  dec_emb = layers.Embedding(vocab, emb_output ,weights= [embedding_matrix], mask_zero = True,input_length=input_shape,trainable=False)(dec_input)\n","  dec_lstm, dec_state_h,dec_state_c = layers.LSTM(units=lstm_units,return_sequences=True,return_state=True)(dec_emb,initial_state= enc_states)\n","  return dec_lstm, dec_state_h,dec_state_c"],"id":"IqiHNviYsD-A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0nbxKJ30aa3"},"source":["enc_input2 = layers.Input(shape=(35))\n","enc_lstm , enc_state_h,enc_state_c = encoder2(35,len(tk_inp.word_index)+1 , 300 ,256, enc_input2 ,embedding_matrix_input)\n","\n","dec_input2 = layers.Input(shape = (35))\n","dec_lstm , dec_state_h,dec_state_c = decoder2(35,len(tk_out.word_index)+1 , 300 , 256 , [enc_state_h,enc_state_c],dec_input2,embedding_matrix_output)\n","\n","dense2 = layers.Dense(len(tk_out.word_index)+1,activation=\"softmax\")(dec_lstm)\n","\n","model2  = Model(inputs=[enc_input2,dec_input2],outputs=dense2)"],"id":"h0nbxKJ30aa3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM7FO68hBsO5"},"source":["callback2 =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001)\n","]\n","\n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","model2.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"],"id":"PM7FO68hBsO5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnHCGdPh0aVU"},"source":["history = model2.fit(train_dataloader,steps_per_epoch=train_steps,epochs=50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback2)"],"id":"XnHCGdPh0aVU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJdPGMY3ByAr"},"source":["model2.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")"],"id":"gJdPGMY3ByAr","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3nb7CQ7d-AW"},"source":["## PREDICTIONS FOR TEST DATASET"],"id":"Y3nb7CQ7d-AW"},{"cell_type":"code","metadata":{"id":"vbeFQ9MBeFrB"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model2))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"],"id":"vbeFQ9MBeFrB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvsZIIoweFrC"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model2))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"],"id":"pvsZIIoweFrC","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhBx5c_PkCxp"},"source":["## Inference time"],"id":"dhBx5c_PkCxp"},{"cell_type":"code","metadata":{"id":"KjWX1FFOkCd6"},"source":["%%time\n","predict(df_test.enc_input.values[50],model2)"],"id":"KjWX1FFOkCd6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2ycaBOx-Eqz"},"source":["# VALIDATION BELU SCORE\n","BLEU_w2v = []\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = predict(str(i.enc_input),model2).split()\n","        act = [str(i.dec_output).split()]\n","        b =bleu.sentence_bleu(act,pred)\n","        BLEU_w2v.append(b)\n","    except:\n","        continue"],"id":"U2ycaBOx-Eqz","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9YYX7PuB_7m"},"source":["print(\"Triain BELU Score = \",np.mean(BLEU_w2v))"],"id":"p9YYX7PuB_7m","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7Xdbv23_klT"},"source":["## Model 3 -EMBEDDINGS WITH  FAST TEXT"],"id":"F7Xdbv23_klT"},{"cell_type":"code","metadata":{"id":"E66YeYZ8-EyY"},"source":["!wget --header=\"Host: dl.fbaipublicfiles.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://fasttext.cc/\" \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\" -c -O 'crawl-300d-2M-subword.zip'"],"id":"E66YeYZ8-EyY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESWMk3DgCkBy"},"source":["import io\n","import zipfile\n","from gensim.models import KeyedVectors\n","from gensim.models import FastText as fText"],"id":"ESWMk3DgCkBy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxHpRfaVCwl1"},"source":["z = zipfile.ZipFile(\"crawl-300d-2M-subword.zip\" , 'r')\n","z.extractall(\"ft/\")"],"id":"mxHpRfaVCwl1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OajQVjA01q3c"},"source":["ft_model = fText.load_fasttext_format(\"/content/ft/crawl-300d-2M-subword\",encoding='latin1')"],"id":"OajQVjA01q3c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8IFUK2N_mch"},"source":["embedding_matrix_ft_input = np.zeros((len(tk_inp.word_index)+1,300))\n","for word,index in tqdm(tk_inp.word_index.items()) :\n","    try:\n","        vec = en_model[word]\n","        embedding_matrix_ft_input[index] = vec\n","    except:\n","        continue"],"id":"j8IFUK2N_mch","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adzkqliTx0wJ"},"source":["embedding_matrix_ft_out = np.zeros((len(tk_out.word_index)+1,300))\n","for word,index in tqdm(tk_out.word_index.items()) :\n","    try:\n","        vec = en_model[word]\n","        embedding_matrix_ft_out[index] = vec\n","    except:\n","        continue"],"id":"adzkqliTx0wJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCJpT97hQsM7"},"source":["## MODEL 3 ARCHTECTURE"],"id":"GCJpT97hQsM7"},{"cell_type":"code","metadata":{"id":"G4lx2Ciwy692"},"source":["def encoder3(input_shape,vocab, emb_output, lstm_units, enc_input,embedding_matrix):\n","  \n","  enc_emb = layers.Embedding(vocab, emb_output,weights= [embedding_matrix] ,mask_zero = True,input_length=input_shape,trainable=False)(enc_input)\n","  enc_lstm , enc_state_h,enc_state_c = layers.LSTM(units= lstm_units,return_sequences=True,return_state=True)(enc_emb)\n","  return enc_lstm , enc_state_h,enc_state_c\n","\n","def decoder3(input_shape,vocab, emb_output, lstm_units,enc_states, dec_input,embedding_matrix):\n","  \n","  dec_emb = layers.Embedding(vocab, emb_output ,weights= [embedding_matrix], mask_zero = True,input_length=input_shape,trainable=False)(dec_input)\n","  dec_lstm, dec_state_h,dec_state_c = layers.LSTM(units=lstm_units,return_sequences=True,return_state=True)(dec_emb,initial_state= enc_states)\n","  return dec_lstm, dec_state_h,dec_state_c"],"id":"G4lx2Ciwy692","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3L0GwdQ3843w"},"source":["enc_input3 = layers.Input(shape=(35))\n","enc_lstm3 , enc_state_h3,enc_state_c3 = encoder3(35,len(tk_inp.word_index)+1 , 300 ,256, enc_input3 ,embedding_matrix_ft_input)\n","\n","dec_input3 = layers.Input(shape = (35))\n","dec_lstm3 , dec_state_h3,dec_state_c3 = decoder3(35,len(tk_out.word_index)+1 , 300 , 256 , [enc_state_h3,enc_state_c3],dec_input3,embedding_matrix_ft_out)\n","\n","dense3 = layers.Dense(len(tk_out.word_index)+1,activation=\"softmax\")(dec_lstm3)\n","\n","model3  = Model(inputs=[enc_input3,dec_input3],outputs=dense3)"],"id":"3L0GwdQ3843w","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCRGmyofFxFd"},"source":["callback3 =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001)\n","]\n","\n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","model3.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"],"id":"lCRGmyofFxFd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAZeO5Wx85BH"},"source":["history = model3.fit(train_dataloader,steps_per_epoch=train_steps,epochs=50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback3)"],"id":"aAZeO5Wx85BH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oiqmi8AIGEBw"},"source":["model3.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")"],"id":"Oiqmi8AIGEBw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLzZTWdZh3KX"},"source":["## PREDICTIONS FOR TEST DATASET"],"id":"eLzZTWdZh3KX"},{"cell_type":"code","metadata":{"id":"yabkFashiB3t"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model3))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"],"id":"yabkFashiB3t","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YY9sLw4ZiB3w"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model3))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"],"id":"YY9sLw4ZiB3w","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmlU3hmnkKlb"},"source":["## Inference Time"],"id":"VmlU3hmnkKlb"},{"cell_type":"code","metadata":{"id":"ICIbeKHekKXw"},"source":["%%time\n","predict(df_test.enc_input.values[50],model3)"],"id":"ICIbeKHekKXw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uCl3cIF_zj2"},"source":["# VALIDATION BELU SCORE\n","BLEU_val_ft = []\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    pred = predict(str(i.enc_input),model3).split()\n","    act = [str(i.dec_output).split()]\n","    b =bleu.sentence_bleu(act,pred)\n","    BLEU_val_ft.append(b)"],"id":"_uCl3cIF_zj2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"110jtgdZ_zj3"},"source":["print(\"Triain BELU Score = \",np.mean(BLEU_val_ft))"],"id":"110jtgdZ_zj3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXVCnjvQLZFh"},"source":["## Model Comparison"],"id":"cXVCnjvQLZFh"},{"cell_type":"code","metadata":{"id":"2b6788e8"},"source":["s1 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")\n","s2 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")\n","s3 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")\n","s4 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")\n","s5 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")\n","s6 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/multi_layered_word/besh.h5\")\n","s7 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")\n","s8 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_gernal/best.h5\")\n","s9 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")\n","s10 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","s11 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_general/best.h5\")\n","s12 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_concat/best.h5\")\n"],"id":"2b6788e8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw5u7rGsb3Ec"},"source":["df_comp = pd.DataFrame()\n","df_comp[\"Model\"] = [\"Encoder Decoder(Char Level)\",\"Encoder Decoder\",\"Encoder Decoder\",\"Encoder Decoder\",\"Bidirectional Encoder Decoder\",\"Multilayered Encoder Decoder\",\"Attention Dot Model\",\"Attention Gernal Model\",\"Attention Concat Model\",\"Monotonic Attention Dot Model\",\"Monotonic Attention Gernal Model\",\"Monotonic Attention Concat Model\"]\n","df_comp[\"Embedding\"] = [\"One Hot Encoding\",\"Trainable Embedding\" , \"Pretrained Word2Vec \" ,\"Fast Text\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\"]\n","df_comp[\"BLEU Score(Greedy Search)\"] = [0.3139,0.4603,0.4453,0.4569,0.4509,0.4527, 0.5055,0.5545,0.5388,0.5469,0.5514,0.5348]\n","df_comp[\"BLEU Score(Beam Search)\"] = [\"-\",\"-\",\"-\",\"-\",0.4561,0.4557,0.5411,0.5324,0.5671,\"-\",\"-\",\"-\"]\n","df_comp[\"Model Size(bytes)\"] = [ s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12]\n","df_comp[\"Model Parameters\"] = [\"616,488\t\",\"26,363,578\" , \"8,158,378\", \"8,158,378\",\"35,018,938\" ,\" 28,464,826\",\"33,353,914\",\"33,419,706\",\"33,485,755\",\"33,353,914\",\"33,419,706\",\"33,485,755\"]\n","df_comp[\"Inference Time(ms)\"] = [143,92.3 , 94.5 , 93.7,250,311,157,164,189,164,179,176]\n","df_comp"],"id":"Dw5u7rGsb3Ec","execution_count":null,"outputs":[]}]}