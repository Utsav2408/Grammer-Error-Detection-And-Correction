{"cells":[{"cell_type":"markdown","metadata":{"id":"D1KM7K8WQ27K"},"source":["## FINAL FILE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw6W8TadAmyX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm \n","import tensorflow as tf\n","from  tensorflow.keras.preprocessing.sequence import pad_sequences\n","from  sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import pickle\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","import nltk.translate.bleu_score as bleu\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guS0Z8RuCwHj"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5xOP5jaCw1J"},"outputs":[],"source":["df= pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/cs2/processed_data.csv\")\n","df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n","df[\"dec_output\"] = df.dec_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eY-aByiqUf9c"},"outputs":[],"source":["df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))\n","## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n","np.random.seed(5) \n","df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]"]},{"cell_type":"markdown","metadata":{"id":"UwqSzwYPWcIw"},"source":["### Note: The model that i am using in final file is Monotonic Attention model which computes score values by dot product"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Agx9DPeEYk0"},"outputs":[],"source":["def function1(text):\n","    starttime1 = datetime.now()\n","    ## TEXT PREPROCESSING ########\n","    def remove_spaces(text):\n","        text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n","        text = re.sub(r\" \\,\",\",\",text)\n","        text = re.sub(r\" \\.+\",\".\",text)\n","        text = re.sub(r\" \\!+\",\"!\",text)\n","        text = re.sub(r\" \\?+\",\"?\",text)\n","        text = re.sub(\" n't\",\"n't\",text)\n","        text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n","        \n","        return text\n","\n","    def decontract(text):\n","        text = re.sub(r\"won\\'t\", \"will not\", text)\n","        text = re.sub(r\"can\\'t\", \"can not\", text)\n","        text = re.sub(r\"n\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'re\", \" are\", text)\n","        text = re.sub(r\"\\'s\", \" is\", text)\n","        text = re.sub(r\"\\'d\", \" would\", text)\n","        text = re.sub(r\"\\'ll\", \" will\", text)\n","        text = re.sub(r\"\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'ve\", \" have\", text)\n","        text = re.sub(r\"\\'m\", \" am\", text)\n","        return text\n","    def preprocess(text):\n","        text = re.sub(\"\\n\",\"\",text)\n","        text = remove_spaces(text)   # REMOVING UNWANTED SPACES\n","        text = re.sub(r\"\\.+\",\".\",text)\n","        text = re.sub(r\"\\!+\",\"!\",text)\n","        text = decontract(text)    # DECONTRACTION\n","        text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n","        text = text.lower()\n","        return text\n","    \n","    text = preprocess(text)\n","\n","\n","    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n","    tk_inp = pickle.load(open(\"/content/drive/MyDrive/ColabNotebooks/cs2/final/tk_inp\",\"rb\"))\n","    seq = tk_inp.texts_to_sequences([text])\n","    # PADDING THE SEQUENCES\n","    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n","    tk_out = pickle.load(open(\"/content/drive/MyDrive/ColabNotebooks/cs2/final/tk_out\",\"rb\"))\n","\n","    #### Model #########################\n","    class Encoder(tf.keras.layers.Layer):\n","\n","        '''\n","        Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n","        '''\n","        \n","        def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.input_length = input_length\n","            # INITIALIZING THE REQUIRED VARIABLES\n","            self.batch_size=batch_size # BATHCH SIZE\n","            self.enc_units = enc_units # ENCODER UNITS\n","\n","            # EMBEDDING LAYER\n","            self.embedding= layers.Embedding(vocab_size ,emb_dims) \n","            # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n","            self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n","            \n","        def call(self, enc_input , states):\n","            '''\n","            This function takes a sequence input and the initial states of the encoder.\n","            Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n","            returns -- encoder_output, last time step's hidden and cell state\n","            '''\n","            # FORMING THE EMBEDDED VECTOR \n","            emb = self.embedding(enc_input)\n","            # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n","            enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n","            #RETURNING THE OUTPUT OF LSTM LAYER\n","            return enc_output,state_h,state_c \n","        \n","        def initialize(self,batch_size):\n","\n","            '''\n","            Given a batch size it will return intial hidden state and intial cell state.\n","            If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n","            '''\n","            return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))\n","        def get_config(self):\n","            config = super(Encoder, self).get_config()\n","            config.update({\"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims, \"enc_units\":self.enc_units,\"input_length\":self.input_length,\"batch_size\":self.batch_size})\n","            return config\n","\n","    class Monotonic_Attention(tf.keras.layers.Layer):\n","        '''THIS FUNCTION RETURNS THE CONTEXT VECTOR AND ATTENTION WEIGHTS (ALPHA VALUES)'''\n","        def __init__(self,units,att_mode):\n","            super().__init__()\n","            self.units = units\n","            self.att_mode = att_mode\n","            # INITIALIZING THE DENSE LAYER W1\n","            self.W1 = layers.Dense(units)\n","            # INITIALIZING THE DENSE LAYER W2\n","            self.W2 = layers.Dense(units)\n","            # INITIALIZING THE DENSE LAYER V\n","            self.v = layers.Dense(1)\n","            self.mode = att_mode\n","            \n","        def call(self,enc_output,dec_state,prev_att):\n","            # HERE WE ARE COMPUTING THE SCORE \n","\n","            if self.mode == \"dot\":\n","            # FINDING THE SCORE FOR DOT MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=-1)\n","                score = tf.matmul(enc_output,dec_state)\n","                score = tf.squeeze(score, [2])\n","                \n","                \n","            if self.mode == \"general\":\n","            # FINDING THE SCORE FOR GENERAL MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=-1)\n","                dense_output = self.W1(enc_output)\n","                score = tf.matmul(dense_output , dec_state)\n","                score = tf.squeeze(score, [2])\n","                \n","                \n","            if self.mode == \"concat\":\n","            # FINDING THE SCORE FOR CONCAT MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=1)\n","                score = self.v(tf.nn.tanh(\n","                    self.W1(dec_state)+ self.W2(enc_output)))\n","                score = tf.squeeze(score, [2])\n","            \n","            # AFTER THE SOCRES ARE COMPUTED THE SIGMOID IS USED ON IT\n","            probabilities = tf.sigmoid(score)\n","\n","            # ATTENTION WEIGHTS FOR PRESENT TIME STEP\n","            probabilities = probabilities*tf.cumsum(tf.squeeze(prev_att,-1), axis=1)\n","            attention = probabilities*tf.math.cumprod(1-probabilities, axis=1, exclusive=True)\n","            attention = tf.expand_dims(attention,axis=-1)\n","            \n","            # CONTEXT VECTOR\n","            context_vec  =  attention  * enc_output\n","            context_vec = tf.reduce_sum(context_vec,axis=1)\n","            \n","            # RETURN CONTEXT VECTOR AND ATTENTION\n","            return context_vec, attention\n","        def get_config(self):\n","            config = super(Monotonic_Attention, self).get_config()\n","            config.update({\"units\":self.units,\"att_mode\":self.att_mode})\n","            return config\n","\n","    class Onestepdecoder(tf.keras.Model):\n","        '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n","    \n","        def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size, att_mode):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.dec_units = dec_units\n","            self.input_len = input_len\n","            self.att_units = att_units\n","            self.batch_size = batch_size\n","            self.att_mode = att_mode\n","\n","            # INTITALIZING THE REQUIRED VARIABLES\n","            # EMBEDDING LAYERS\n","            self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n","            # ATTENTION LAYER\n","            self.att = Monotonic_Attention(att_units,att_mode)\n","            # LSTM LAYER\n","            self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n","            # DENSE LAYER\n","            self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n","\n","        def call(self, encoder_output , input , state_h,state_c,previous_attention):\n","            # FORMING THE EMBEDDED VECTOR FOR THE WORD\n","            # (32,1)=>(32,1,12)\n","            emb = self.emb(input)\n","\n","            dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n","\n","            # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n","            context_vec,alphas = self.att(encoder_output,dec_state_h,previous_attention)\n","            \n","            # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n","            dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n","            \n","            # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n","            fc = self.dense(dense_input)\n","            \n","            # RETURNING THE OUTPUT\n","            return fc , dec_state_h , dec_state_c , alphas\n","\n","\n","        def get_config(self):\n","            config=({ \"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims,\"dec_units\": self.dec_units,\"input_len\": self.input_len,\"att_units\":self.att_units,\"batch_size\":self.batch_size, \"att_mode\":self.att_mode})\n","            return config\n","\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","\n","    class Decoder(tf.keras.Model):\n","        '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n","        def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size,att_mode):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.dec_units = dec_units\n","            \n","            self.att_units = att_units\n","            self.batch_size = batch_size\n","            self.att_mode = att_mode\n","\n","            # INITIALIZING THE VARIABLES\n","            # LENGTH OF INPUT SENTENCE\n","            self.input_len = input_len\n","            # ONE STEP DECODER\n","            self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size,att_mode)\n","\n","        def call(self,dec_input,enc_output,state_h,state_c,initial_attention):\n","            # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n","            current_state_h = state_h \n","            current_state_c = state_c\n","            previous_attention = initial_attention\n","            # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n","            pred = []\n","            # THIS STORED THE ALPHA VALUES\n","            alpha_values = []\n","            # FOR EACH WORD IN THE INPUT SENTENCE\n","            for i in range(self.input_len):\n","                \n","                # CURRENT WORD TO INPUT TO ONE STEP DECODER\n","                current_vec = dec_input[:,i]\n","\n","                # EXPANDING THE DIMENSION FOR THE WORD\n","                current_vec = tf.expand_dims(current_vec,axis=-1)\n","\n","                # PERFORMING THE ONE STEP DECODER OPERATION \n","                dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c,previous_attention)\n","\n","                #UPDATING THE CURRENT STATE_H\n","                current_state_h = dec_state_h\n","                current_state_c = dec_state_c\n","                previous_attention = alphas\n","                \n","                #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n","                pred.append(dec_output)\n","\n","                # APPENDING THE ALPHA VALUES\n","                alpha_values.append(alphas)\n","                \n","            # CONCATINATING ALL THE VALUES IN THE LIST\n","            output = tf.concat(pred,axis=1)\n","            # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n","            alpha_values = tf.concat(alpha_values,axis = -1)\n","            # RETURNING THE OUTPUT\n","            return output , alpha_values\n","        def get_config(self):\n","          config = ({ \"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims,\"dec_units\": self.dec_units, \"input_len\":self.input_len,\"att_units\":self.att_units,\"batch_size\":self.batch_size,\"att_model\":self.att_mode})\n","          return config\n","\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","    class encoder_decoder(tf.keras.Model):\n","        '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n","        def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n","                dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size,att_mode):\n","            # INITAILIZING ALL VARIABLES\n","            super().__init__()\n","            self.enc_vocab_size= enc_vocab_size\n","            self.enc_emb_dim=enc_emb_dim\n","            self.enc_units= enc_units\n","            self.enc_input_length =enc_input_length\n","            self.dec_vocab_size=dec_vocab_size\n","            self.dec_emb_dim=dec_emb_dim\n","            self.dec_units=dec_units\n","            self.dec_input_length =dec_input_length\n","            self.att_units=att_units\n","            self.att_mode=att_mode\n","\n","            # BATCH SIZE\n","            self.batch_size = batch_size\n","            # INITIALIZING ENCODER LAYER\n","            self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n","            # INITALIZING DECODER LAYER\n","            self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size,att_mode)\n","            self.input_len = enc_input_length\n","            \n","            \n","        def call(self,data):\n","            # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n","            # INPUT1 IS FOR ENCODER\n","            # INPUT2 IS FOR DECODER\n","            inp1 , inp2 = data\n","            # PASSING THE INPUT1 TO ENCODER LAYER\n","            enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n","            # PASSING INPUT2 TO THE DECODER LAYER\n","            initial_attention = np.zeros(shape = (self.batch_size,self.input_len,1),dtype=\"float32\")\n","            initial_attention[:,1] = 1 \n","            dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c ,initial_attention)\n","            # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n","            return dec_output\n","\n","        def get_config(self):\n","            config = ({\"enc_vocab_size\":self.enc_vocab_size, \n","                      \"enc_emb_dim\":self.enc_emb_dim,\"enc_units\":self.enc_units,\"enc_input_length\":self.enc_input_length,\\\n","                \"dec_vocab_size\":self.dec_vocab_size,\n","                \"dec_emb_dim\":self.dec_emb_dim,\n","                \"dec_units\":self.dec_units,\n","                \"dec_input_length\":self.dec_input_length ,\\\n","                \"att_units\":self.att_units, \"batch_size\":self.batch_size,\"att_mode\":self.att_mode})\n","            return config\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","\n","    # INITAILZING THE MODEL\n","    model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n","                            enc_emb_dim = 300,\n","                            enc_units=256,enc_input_length=35,\n","                            dec_vocab_size =len(tk_out.word_index)+1,\n","                            dec_emb_dim =300,\n","                            dec_units=256,\n","                            dec_input_length = 35,\n","                            \n","                            att_units=256,\n","                            batch_size=512,\n","                              att_mode = \"dot\")\n","    model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')\n","    model.build([(512,35),(512,35)])\n","    \n","    model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","\n","    print(\"####### Model Loaded ###########\")\n","    time2 = datetime.now()\n","    print(\"Loading time = \", (time2-starttime1).total_seconds(),\"seconds\")\n","\n","    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n","    state = model.layers[0].initialize(1)\n","\n","    # GETTING THE ENCODED OUTPUT\n","    enc_output,state_h,state_c= model.layers[0](seq,state)\n","    # VARIABLE TO STORE PREDICTED SENTENCE\n","    pred = []\n","    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n","    input_state_h = state_h\n","    input_state_c = state_c\n","    prev_attention = np.zeros(shape = (1,20,1),dtype=\"float32\")\n","    prev_attention[:,1] = 1 \n","    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n","    current_vec = tf.ones((1,1))\n","    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n","    alpha_values = []\n","\n","    for i in range(20):\n","        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n","        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c,prev_attention)\n","        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n","        alpha_values.append(alphas)\n","         # UPDATING THE CURRENT VECTOR \n","        current_vec = np.argmax(fc , axis = -1)\n","         # UPDATING THE INPUT STATE\n","        input_state_h = dec_state_h\n","        input_state_c = dec_state_c\n","        prev_attention = alphas\n","        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n","        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n","              break\n","    # JOINING THE PREDICTED WORDS\n","    pred_sent = \" \".join(pred)\n","    # CONCATINATING ALL THE ALPHA VALUES\n","    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n","    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n","    print(\"Predicted Output\",pred_sent)\n","    print(\"=\"*50)\n","    #### Plot for alphas #####\n","    def plot( input_sent , output_sent , alpha ) :\n","        '''THIS FUNCTION PLOTS THE ALPHA VALUES IN FORM OF HEAT MAPS'''\n","      \n","        input_words = input_sent.split() # SPLITTING THE INPUT SENTENCE\n","        output_words = output_sent.split() # SPLITTING THE OUTPUT SENTENCE\n","        \n","        fig, ax = plt.subplots()\n","        sns.set_style(\"darkgrid\")\n","        # HEAT MAP WITH ALPHA VALURS \n","        # X LABELS ARE THE OUTPUT WORDS \n","        # T LABELS ARE THE INPUT WORDS\n","        sns.heatmap(alpha[:len(input_words),:], xticklabels= output_words , yticklabels=input_words,linewidths=0.01)\n","        # PLACING THE TICKS ON  THE TOP\n","        ax.xaxis.tick_top( ) \n","        plt.show()\n","\n","    plot(text,pred_sent,alpha_values)\n","    print(\"=\"*50)\n","    print(\"Prediction time = \", (datetime.now()-time2).total_seconds(),\"seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbQpmesaCw_g"},"outputs":[],"source":["function1(df_test.enc_input.values[50])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJWairFBVXnv"},"outputs":[],"source":["function1(df_test.enc_input.values[500])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ltQSNP7He5y"},"outputs":[],"source":["def function2(text,actual_output):\n","\n","    ## TEXT PREPROCESSING ### \n","    def remove_spaces(text):\n","        text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n","        text = re.sub(r\" \\,\",\",\",text)\n","        text = re.sub(r\" \\.+\",\".\",text)\n","        text = re.sub(r\" \\!+\",\"!\",text)\n","        text = re.sub(r\" \\?+\",\"?\",text)\n","        text = re.sub(\" n't\",\"n't\",text)\n","        text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n","        \n","        return text\n","\n","    def decontract(text):\n","        text = re.sub(r\"won\\'t\", \"will not\", text)\n","        text = re.sub(r\"can\\'t\", \"can not\", text)\n","        text = re.sub(r\"n\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'re\", \" are\", text)\n","        text = re.sub(r\"\\'s\", \" is\", text)\n","        text = re.sub(r\"\\'d\", \" would\", text)\n","        text = re.sub(r\"\\'ll\", \" will\", text)\n","        text = re.sub(r\"\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'ve\", \" have\", text)\n","        text = re.sub(r\"\\'m\", \" am\", text)\n","        return text\n","    def preprocess(text):\n","        text = re.sub(\"\\n\",\"\",text)\n","        text = remove_spaces(text)   # REMOVING UNWANTED SPACES\n","        text = re.sub(r\"\\.+\",\".\",text)\n","        text = re.sub(r\"\\!+\",\"!\",text)\n","        text = decontract(text)    # DECONTRACTION\n","        text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n","        text = text.lower()\n","        return text\n","    \n","    text = preprocess(text)\n","    \n","\n","\n","    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n","    tk_inp = pickle.load(open(\"/content/drive/MyDrive/ColabNotebooks/cs2/final/tk_inp\",\"rb\"))\n","    seq = tk_inp.texts_to_sequences([text])\n","    # PADDING THE SEQUENCES\n","    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n","    tk_out = pickle.load(open(\"/content/drive/MyDrive/ColabNotebooks/cs2/final/tk_out\",\"rb\"))\n","\n","    #### Model #########################\n","    class Encoder(tf.keras.layers.Layer):\n","\n","        '''\n","        Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n","        '''\n","        \n","        def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.input_length = input_length\n","            # INITIALIZING THE REQUIRED VARIABLES\n","            self.batch_size=batch_size # BATHCH SIZE\n","            self.enc_units = enc_units # ENCODER UNITS\n","\n","            # EMBEDDING LAYER\n","            self.embedding= layers.Embedding(vocab_size ,emb_dims) \n","            # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n","            self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n","            \n","        def call(self, enc_input , states):\n","            '''\n","            This function takes a sequence input and the initial states of the encoder.\n","            Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n","            returns -- encoder_output, last time step's hidden and cell state\n","            '''\n","            # FORMING THE EMBEDDED VECTOR \n","            emb = self.embedding(enc_input)\n","            # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n","            enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n","            #RETURNING THE OUTPUT OF LSTM LAYER\n","            return enc_output,state_h,state_c \n","        \n","        def initialize(self,batch_size):\n","\n","            '''\n","            Given a batch size it will return intial hidden state and intial cell state.\n","            If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n","            '''\n","            return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))\n","        def get_config(self):\n","            config = super(Encoder, self).get_config()\n","            config.update({\"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims, \"enc_units\":self.enc_units,\"input_length\":self.input_length,\"batch_size\":self.batch_size})\n","            return config\n","\n","    class Monotonic_Attention(tf.keras.layers.Layer):\n","        '''THIS FUNCTION RETURNS THE CONTEXT VECTOR AND ATTENTION WEIGHTS (ALPHA VALUES)'''\n","        def __init__(self,units,att_mode):\n","            super().__init__()\n","            self.units = units\n","            self.att_mode = att_mode\n","            # INITIALIZING THE DENSE LAYER W1\n","            self.W1 = layers.Dense(units)\n","            # INITIALIZING THE DENSE LAYER W2\n","            self.W2 = layers.Dense(units)\n","            # INITIALIZING THE DENSE LAYER V\n","            self.v = layers.Dense(1)\n","            self.mode = att_mode\n","            \n","        def call(self,enc_output,dec_state,prev_att):\n","            # HERE WE ARE COMPUTING THE SCORE \n","\n","            if self.mode == \"dot\":\n","            # FINDING THE SCORE FOR DOT MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=-1)\n","                score = tf.matmul(enc_output,dec_state)\n","                score = tf.squeeze(score, [2])\n","                \n","                \n","            if self.mode == \"general\":\n","            # FINDING THE SCORE FOR GENERAL MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=-1)\n","                dense_output = self.W1(enc_output)\n","                score = tf.matmul(dense_output , dec_state)\n","                score = tf.squeeze(score, [2])\n","                \n","                \n","            if self.mode == \"concat\":\n","            # FINDING THE SCORE FOR CONCAT MODEL\n","                dec_state =  tf.expand_dims(dec_state,axis=1)\n","                score = self.v(tf.nn.tanh(\n","                    self.W1(dec_state)+ self.W2(enc_output)))\n","                score = tf.squeeze(score, [2])\n","            \n","            # AFTER THE SOCRES ARE COMPUTED THE SIGMOID IS USED ON IT\n","            probabilities = tf.sigmoid(score)\n","\n","            # ATTENTION WEIGHTS FOR PRESENT TIME STEP\n","            probabilities = probabilities*tf.cumsum(tf.squeeze(prev_att,-1), axis=1)\n","            attention = probabilities*tf.math.cumprod(1-probabilities, axis=1, exclusive=True)\n","            attention = tf.expand_dims(attention,axis=-1)\n","            \n","            # CONTEXT VECTOR\n","            context_vec  =  attention  * enc_output\n","            context_vec = tf.reduce_sum(context_vec,axis=1)\n","            \n","            # RETURN CONTEXT VECTOR AND ATTENTION\n","            return context_vec, attention\n","        def get_config(self):\n","            config = super(Monotonic_Attention, self).get_config()\n","            config.update({\"units\":self.units,\"att_mode\":self.att_mode})\n","            return config\n","\n","    class Onestepdecoder(tf.keras.Model):\n","        '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n","    \n","        def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size, att_mode):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.dec_units = dec_units\n","            self.input_len = input_len\n","            self.att_units = att_units\n","            self.batch_size = batch_size\n","            self.att_mode = att_mode\n","\n","            # INTITALIZING THE REQUIRED VARIABLES\n","            # EMBEDDING LAYERS\n","            self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n","            # ATTENTION LAYER\n","            self.att = Monotonic_Attention(att_units,att_mode)\n","            # LSTM LAYER\n","            self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n","            # DENSE LAYER\n","            self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n","\n","        def call(self, encoder_output , input , state_h,state_c,previous_attention):\n","            # FORMING THE EMBEDDED VECTOR FOR THE WORD\n","            # (32,1)=>(32,1,12)\n","            emb = self.emb(input)\n","\n","            dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n","\n","            # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n","            context_vec,alphas = self.att(encoder_output,dec_state_h,previous_attention)\n","            \n","            # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n","            dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n","            \n","            # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n","            fc = self.dense(dense_input)\n","            \n","            # RETURNING THE OUTPUT\n","            return fc , dec_state_h , dec_state_c , alphas\n","\n","\n","        def get_config(self):\n","            config=({ \"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims,\"dec_units\": self.dec_units,\"input_len\": self.input_len,\"att_units\":self.att_units,\"batch_size\":self.batch_size, \"att_mode\":self.att_mode})\n","            return config\n","\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","\n","    class Decoder(tf.keras.Model):\n","        '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n","        def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size,att_mode):\n","            super().__init__()\n","            self.vocab_size = vocab_size\n","            self.emb_dims = emb_dims\n","            self.dec_units = dec_units\n","            \n","            self.att_units = att_units\n","            self.batch_size = batch_size\n","            self.att_mode = att_mode\n","\n","            # INITIALIZING THE VARIABLES\n","            # LENGTH OF INPUT SENTENCE\n","            self.input_len = input_len\n","            # ONE STEP DECODER\n","            self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size,att_mode)\n","\n","        def call(self,dec_input,enc_output,state_h,state_c,initial_attention):\n","            # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n","            current_state_h = state_h \n","            current_state_c = state_c\n","            previous_attention = initial_attention\n","            # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n","            pred = []\n","            # THIS STORED THE ALPHA VALUES\n","            alpha_values = []\n","            # FOR EACH WORD IN THE INPUT SENTENCE\n","            for i in range(self.input_len):\n","                \n","                # CURRENT WORD TO INPUT TO ONE STEP DECODER\n","                current_vec = dec_input[:,i]\n","\n","                # EXPANDING THE DIMENSION FOR THE WORD\n","                current_vec = tf.expand_dims(current_vec,axis=-1)\n","\n","                # PERFORMING THE ONE STEP DECODER OPERATION \n","                dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c,previous_attention)\n","\n","                #UPDATING THE CURRENT STATE_H\n","                current_state_h = dec_state_h\n","                current_state_c = dec_state_c\n","                previous_attention = alphas\n","                \n","                #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n","                pred.append(dec_output)\n","\n","                # APPENDING THE ALPHA VALUES\n","                alpha_values.append(alphas)\n","                \n","            # CONCATINATING ALL THE VALUES IN THE LIST\n","            output = tf.concat(pred,axis=1)\n","            # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n","            alpha_values = tf.concat(alpha_values,axis = -1)\n","            # RETURNING THE OUTPUT\n","            return output , alpha_values\n","        def get_config(self):\n","          config = ({ \"vocab_size\":self.vocab_size,\"emb_dims\":self.emb_dims,\"dec_units\": self.dec_units, \"input_len\":self.input_len,\"att_units\":self.att_units,\"batch_size\":self.batch_size,\"att_model\":self.att_mode})\n","          return config\n","\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","    class encoder_decoder(tf.keras.Model):\n","        '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n","        def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n","                dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size,att_mode):\n","            # INITAILIZING ALL VARIABLES\n","            super().__init__()\n","            self.enc_vocab_size= enc_vocab_size\n","            self.enc_emb_dim=enc_emb_dim\n","            self.enc_units= enc_units\n","            self.enc_input_length =enc_input_length\n","            self.dec_vocab_size=dec_vocab_size\n","            self.dec_emb_dim=dec_emb_dim\n","            self.dec_units=dec_units\n","            self.dec_input_length =dec_input_length\n","            self.att_units=att_units\n","            self.att_mode=att_mode\n","\n","            # BATCH SIZE\n","            self.batch_size = batch_size\n","            # INITIALIZING ENCODER LAYER\n","            self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n","            # INITALIZING DECODER LAYER\n","            self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size,att_mode)\n","            self.input_len = enc_input_length\n","            \n","            \n","        def call(self,data):\n","            # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n","            # INPUT1 IS FOR ENCODER\n","            # INPUT2 IS FOR DECODER\n","            inp1 , inp2 = data\n","            # PASSING THE INPUT1 TO ENCODER LAYER\n","            enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n","            # PASSING INPUT2 TO THE DECODER LAYER\n","            initial_attention = np.zeros(shape = (self.batch_size,self.input_len,1),dtype=\"float32\")\n","            initial_attention[:,1] = 1 \n","            dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c ,initial_attention)\n","            # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n","            return dec_output\n","\n","        def get_config(self):\n","            config = ({\"enc_vocab_size\":self.enc_vocab_size, \n","                      \"enc_emb_dim\":self.enc_emb_dim,\"enc_units\":self.enc_units,\"enc_input_length\":self.enc_input_length,\\\n","                \"dec_vocab_size\":self.dec_vocab_size,\n","                \"dec_emb_dim\":self.dec_emb_dim,\n","                \"dec_units\":self.dec_units,\n","                \"dec_input_length\":self.dec_input_length ,\\\n","                \"att_units\":self.att_units, \"batch_size\":self.batch_size,\"att_mode\":self.att_mode})\n","            return config\n","        @classmethod\n","        def from_config(cls, config):\n","            return cls(**config)\n","\n","    # INITAILZING THE MODEL\n","    model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n","                            enc_emb_dim = 300,\n","                            enc_units=256,enc_input_length=35,\n","                            dec_vocab_size =len(tk_out.word_index)+1,\n","                            dec_emb_dim =300,\n","                            dec_units=256,\n","                            dec_input_length = 35,\n","                            \n","                            att_units=256,\n","                            batch_size=512,\n","                              att_mode = \"dot\")\n","    model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')\n","    model.build([(512,35),(512,35)])\n","    \n","    model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","\n","\n","    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n","    state = model.layers[0].initialize(1)\n","\n","    # GETTING THE ENCODED OUTPUT\n","    enc_output,state_h,state_c= model.layers[0](seq,state)\n","    # VARIABLE TO STORE PREDICTED SENTENCE\n","    pred = []\n","    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n","    input_state_h = state_h\n","    input_state_c = state_c\n","    prev_attention = np.zeros(shape = (1,20,1),dtype=\"float32\")\n","    prev_attention[:,1] = 1 \n","    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n","    current_vec = tf.ones((1,1))\n","    \n","    \n","\n","    for i in range(20):\n","        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n","        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c,prev_attention)\n","        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n","        \n","         # UPDATING THE CURRENT VECTOR \n","        current_vec = np.argmax(fc , axis = -1)\n","         # UPDATING THE INPUT STATE\n","        input_state_h = dec_state_h\n","        input_state_c = dec_state_c\n","        prev_attention = alphas\n","        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n","        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n","              break\n","    # JOINING THE PREDICTED WORDS\n","    pred_sent = \" \".join(pred)\n","    # CONCATINATING ALL THE ALPHA VALUES\n","\n","    print(\"BLEU Score\",bleu.sentence_bleu([actual_output.split()],pred_sent.split()))\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"wnWAzepVXMIk"},"source":["## Function 2 for some random test datapoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7JBdj89He2V"},"outputs":[],"source":["function2(df_test.enc_input.values[5],df_test.dec_output.values[5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5l8W0LB0CxC3"},"outputs":[],"source":["function2(df_test.enc_input.values[50],df_test.dec_output.values[50])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9ST2w5ySAxr"},"outputs":[],"source":["function2(df_test.enc_input.values[99],df_test.dec_output.values[99])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0rD_WHrhrJo"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}