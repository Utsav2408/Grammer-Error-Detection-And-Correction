{"cells":[{"cell_type":"markdown","id":"9c24b2f4","metadata":{"id":"9c24b2f4"},"source":["# Attention Concat Model"]},{"cell_type":"code","execution_count":null,"id":"c8883b77","metadata":{"id":"c8883b77"},"outputs":[],"source":["## LOADING THE REQUIRED LIBRARIES\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm \n","import tensorflow as tf\n","from  tensorflow.keras.preprocessing.sequence import pad_sequences\n","from  sklearn.model_selection import train_test_split\n","from tqdm import tqdm"]},{"cell_type":"markdown","id":"348ac20c","metadata":{"id":"348ac20c"},"source":["## Loading Dataset"]},{"cell_type":"code","execution_count":null,"id":"f82d14dc","metadata":{"id":"f82d14dc"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"90f10819","metadata":{"id":"90f10819"},"outputs":[],"source":["## LOADING THE PROCESSED DATASET\n","df= pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/cs2/processed_data.csv\")\n","df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n","df[\"dec_output\"] = df.dec_input"]},{"cell_type":"code","execution_count":null,"id":"5c877440","metadata":{"id":"5c877440"},"outputs":[],"source":["## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n","## <start> AS BEGINING TOKEN\n","## <end>  AS END TOKEN\n","\n","df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n","df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" "]},{"cell_type":"markdown","id":"d59409a0","metadata":{"id":"d59409a0"},"source":["## Splitting And Sampling around 100k datapoints"]},{"cell_type":"code","execution_count":null,"id":"e4dd440b","metadata":{"id":"e4dd440b"},"outputs":[],"source":["df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))"]},{"cell_type":"code","execution_count":null,"id":"6eae1467","metadata":{"id":"6eae1467"},"outputs":[],"source":["## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n","df_train ,df_val = train_test_split(df_sampled,test_size=0.2,random_state = 3, stratify = df_sampled.y )"]},{"cell_type":"code","execution_count":null,"id":"8f8c0130","metadata":{"id":"8f8c0130"},"outputs":[],"source":["## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n","df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\""]},{"cell_type":"code","execution_count":null,"id":"e08db2af","metadata":{"id":"e08db2af"},"outputs":[],"source":["## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n","np.random.seed(5) \n","df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]"]},{"cell_type":"markdown","id":"216e03a4","metadata":{"id":"216e03a4"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"id":"6bc234f3","metadata":{"id":"6bc234f3"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"id":"4b740133","metadata":{"id":"4b740133"},"outputs":[],"source":["## TOKENIZER FOR ENCODER INPUT\n","tk_inp = Tokenizer()\n","tk_inp.fit_on_texts(df_train.enc_input.apply(str))"]},{"cell_type":"code","execution_count":null,"id":"89c25e9c","metadata":{"id":"89c25e9c"},"outputs":[],"source":["# TOKENIZER FOR DECODER INPUT\n","tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n","tk_out.fit_on_texts(df_train.dec_input.apply(str))"]},{"cell_type":"markdown","id":"745f1a97","metadata":{"id":"745f1a97"},"source":["## Dataset Loader"]},{"cell_type":"code","execution_count":null,"id":"1adc50fd","metadata":{"id":"1adc50fd"},"outputs":[],"source":["## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n","class Dataset :\n","    def __init__(self, data , tk_inp ,tk_out, max_len):\n","        ## SETTING THE REQUIRED ATTRIBUTES\n","        self.encoder_inp = data[\"enc_input\"].apply(str).values\n","        self.decoder_inp = data[\"dec_input\"].apply(str).values\n","        self.decoder_out = data[\"dec_output\"].apply(str).values\n","        self.tk_inp = tk_inp\n","        self.tk_out = tk_out\n","        self.max_len = max_len\n","        \n","    def __getitem__(self,i):\n","        # INPUT SEQUENCES\n","        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n","        # DECODER INPUT SEQUENCES \n","        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n","        # DECODER INPUT SEQUENCES\n","        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n","        \n","        # PADDING THE ENCODER INPUT SEQUENCES\n","        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING THE DECODER INPUT SEQUENCES\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING DECODER OUTPUT SEQUENCES\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n","        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n","    \n","    def __len__(self):\n","        # RETURN THE LEN OF INPUT ENDODER\n","        return len(self.encoder_inp)"]},{"cell_type":"code","execution_count":null,"id":"425db78d","metadata":{"id":"425db78d"},"outputs":[],"source":["## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n","\n","class Dataloader(tf.keras.utils.Sequence):\n","    def __init__(self,batch_size,dataset):\n","        # INTIALIZING THE REQUIRED VARIABLES \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.totl_points = self.dataset.encoder_inp.shape[0]\n","        \n","    def __getitem__(self,i):\n","        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n","        start = i * self.batch_size\n","        stop = (i+1)*self.batch_size\n","        \n","        # PLACEHOLDERS FOR BATCHED DATA\n","        batch_enc =[]\n","        batch_dec_input = []\n","        batch_dec_out =[]\n","\n","        for j in range(start,stop): \n","            \n","            a,b,c = self.dataset[j] \n","            batch_enc.append(a[0]) \n","            batch_dec_input.append(b[0])\n","            batch_dec_out.append(c[0]) \n","        \n","        # Conveting list to array   \n","        batch_enc = (np.array(batch_enc)) \n","        batch_dec_input = np.array(batch_dec_input)\n","        batch_dec_out = np.array(batch_dec_out)\n","        \n","        return [batch_enc , batch_dec_input],batch_dec_out\n","    \n","    def __len__(self):\n","        # Returning the number of batches\n","        return int(self.totl_points/self.batch_size)"]},{"cell_type":"markdown","id":"f86101ba","metadata":{"id":"f86101ba"},"source":["###### WE ARE TAKING THE MAXIMUM LENGHT EQUAL TO 35 WHICH IS 99 PERCENTILE OF THE WORD LENGTH DISTRUBUTION"]},{"cell_type":"code","execution_count":null,"id":"8d4bdf0a","metadata":{"id":"8d4bdf0a"},"outputs":[],"source":["# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n","train_dataset = Dataset(df_train,tk_inp,tk_out,35)\n","train_dataloader = Dataloader( batch_size = 512, dataset=train_dataset)\n","\n","# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n","val_dataset = Dataset(df_val , tk_inp,tk_out,35)\n","val_dataloader = Dataloader(batch_size=512 , dataset=val_dataset)"]},{"cell_type":"markdown","id":"f0eb2248","metadata":{"id":"f0eb2248"},"source":["## Attention Model"]},{"cell_type":"code","execution_count":null,"id":"02d7ccee","metadata":{"id":"02d7ccee"},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras import Model"]},{"cell_type":"code","execution_count":null,"id":"30a033c2","metadata":{"id":"30a033c2"},"outputs":[],"source":["## DEFINING THE ENCODER LAYER AS A FUNCTION\n","\n","class Encoder(tf.keras.layers.Layer):\n","\n","\n","    \n","    def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n","        super().__init__()\n","        # INITIALIZING THE REQUIRED VARIABLES\n","        self.batch_size=batch_size # BATHCH SIZE\n","        self.enc_units = enc_units # ENCODER UNITS\n","\n","        # EMBEDDING LAYER\n","        self.embedding= layers.Embedding(vocab_size ,emb_dims) \n","        # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n","        self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n","    def call(self, enc_input , states):\n","      \n","        # FORMING THE EMBEDDED VECTOR \n","        emb = self.embedding(enc_input)\n","        # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n","        enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n","        #RETURNING THE OUTPUT OF LSTM LAYER\n","        return enc_output,state_h,state_c \n","    def initialize(self,batch_size):\n","\n","        return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))"]},{"cell_type":"code","execution_count":null,"id":"c43b9caa","metadata":{"id":"c43b9caa"},"outputs":[],"source":["# THIS IS ATTNETION LAYER FOR DOT MODEL\n","class Attention(tf.keras.layers.Layer):\n","    \n","    def __init__(self,units):\n","        super().__init__()\n","        # INITIALIZING THE DENSE LAYER W1\n","        self.W1 = layers.Dense(units)\n","        # INITIALIZING THE DENSE LAYER W2\n","        self.W2 = layers.Dense(units)\n","        # INITIALIZING THE DENSE LAYER V\n","        self.v = layers.Dense(1)\n","        \n","    def call(self,enc_output,dec_state):\n","        # EXPANDING THE DIMENSION OF DECODER STATE  EG. FROM (16,32) TO (16,32,1)\n","        dec_state =  tf.expand_dims(dec_state,axis=1)\n","        \n","        # FINDING THE SCORE FOR CONCAT MODEL\n","        score = self.v(tf.nn.tanh(\n","            self.W1(dec_state)+ self.W2(enc_output)\n","        ))\n","        # APPLYING SOFTMAX TO THE AXIS 1\n","        # OUPUT SHAPE = (16,13,1)\n","        att_weights = tf.nn.softmax(score,axis=1)\n","        \n","        # CALCULATING THE CONTEXT VECTOR BY FIRST ELEMENTWISE MULTIPLICATION AND THEN ADDING THE AXIS 1\n","        # (16,13,1)*(16,13,32)=(16,13,32)\n","        context_vec  = att_weights* enc_output\n","        \n","        # (16,13,32) SUM AND REDUCE THE DIMENSION AT AXIS 1 => (16,32)\n","        context_vec = tf.reduce_sum(context_vec,axis=1)\n","        \n","        # RETURNING THE CONTEXT VECTOR AND ATTENTION WEIGHTS\n","        return context_vec,att_weights"]},{"cell_type":"code","execution_count":null,"id":"dbca6e0f","metadata":{"id":"dbca6e0f"},"outputs":[],"source":["class Onestepdecoder(tf.keras.Model):\n","    '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n","    \n","    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n","        super().__init__()\n","        # INTITALIZING THE REQUIRED VARIABLES\n","        # EMBEDDING LAYERS\n","        self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n","        # ATTENTION LAYER\n","        self.att = Attention(att_units)\n","        # LSTM LAYER\n","        self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n","        # DENSE LAYER\n","        self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n","\n","    def call(self, encoder_output , input , state_h,state_c):\n","        # FORMING THE EMBEDDED VECTOR FOR THE WORD\n","        # (32,1)=>(32,1,12)\n","        emb = self.emb(input)\n","\n","        dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n","\n","        # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n","        context_vec,alphas = self.att(encoder_output,dec_state_h)\n","        \n","        # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n","        dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n","        \n","        # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n","        fc = self.dense(dense_input)\n","        \n","        # RETURNING THE OUTPUT\n","        return fc , dec_state_h , dec_state_c , alphas"]},{"cell_type":"code","execution_count":null,"id":"86f456e3","metadata":{"id":"86f456e3"},"outputs":[],"source":["class Decoder(tf.keras.Model):\n","    '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n","    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n","        super().__init__()\n","        # INITIALIZING THE VARIABLES\n","        # LENGTH OF INPUT SENTENCE\n","        self.input_len = input_len\n","        # ONE STEP DECODER\n","        self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size)\n","\n","    def call(self,dec_input,enc_output,state_h,state_c):\n","        # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n","        current_state_h = state_h \n","        current_state_c = state_c\n","        # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n","        pred = []\n","        # THIS STORED THE ALPHA VALUES\n","        alpha_values = []\n","        # FOR EACH WORD IN THE INPUT SENTENCE\n","        for i in range(self.input_len):\n","            \n","            # CURRENT WORD TO INPUT TO ONE STEP DECODER\n","            current_vec = dec_input[:,i]\n","\n","            # EXPANDING THE DIMENSION FOR THE WORD\n","            current_vec = tf.expand_dims(current_vec,axis=-1)\n","\n","            # PERFORMING THE ONE STEP DECODER OPERATION \n","            dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c)\n","\n","            #UPDATING THE CURRENT STATE_H\n","            current_state_h = dec_state_h\n","            current_state_c = dec_state_c\n","\n","            #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n","            pred.append(dec_output)\n","\n","            # APPENDING THE ALPHA VALUES\n","            alpha_values.append(alphas)\n","            \n","        # CONCATINATING ALL THE VALUES IN THE LIST\n","        output = tf.concat(pred,axis=1)\n","        # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n","        alpha_values = tf.concat(alpha_values,axis = -1)\n","        # RETURNING THE OUTPUT\n","        return output , alpha_values\n"]},{"cell_type":"code","execution_count":null,"id":"cee51740","metadata":{"id":"cee51740"},"outputs":[],"source":["class encoder_decoder(tf.keras.Model):\n","    '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n","    def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n","             dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size):\n","        # INITAILIZING ALL VARIABLES\n","        super().__init__()\n","        # BATCH SIZE\n","        self.batch_size = batch_size\n","        # INITIALIZING ENCODER LAYER\n","        self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n","        # INITALIZING DECODER LAYER\n","        self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size)\n","\n","    def call(self,data):\n","        # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n","        # INPUT1 IS FOR ENCODER\n","        # INPUT2 IS FOR DECODER\n","        inp1 , inp2 = data\n","        # PASSING THE INPUT1 TO ENCODER LAYER\n","        enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n","        # PASSING INPUT2 TO THE DECODER LAYER\n","        dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c)\n","        # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n","        return dec_output\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3bced5c3","metadata":{"id":"3bced5c3"},"outputs":[],"source":["# INITAILZING THE MODEL\n","model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n","                         enc_emb_dim = 300,\n","                         enc_units=256,enc_input_length=35,\n","                         dec_vocab_size =len(tk_out.word_index)+1,\n","                         dec_emb_dim =300,\n","                         dec_units=256,\n","                         dec_input_length = 35,\n","                         \n","                         att_units=256,\n","                         batch_size=512)"]},{"cell_type":"code","execution_count":null,"id":"8ec04c1a","metadata":{"id":"8ec04c1a"},"outputs":[],"source":["callback =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001),\n","            tf.keras.callbacks.TensorBoard(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/logs/save\",histogram_freq=1)\n","]\n","\n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"]},{"cell_type":"code","execution_count":null,"id":"d50c3791","metadata":{"id":"d50c3791"},"outputs":[],"source":["model.fit(train_dataloader, steps_per_epoch=train_steps,epochs= 50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"]},{"cell_type":"code","execution_count":null,"id":"7f2e4b21","metadata":{"id":"7f2e4b21"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/logs/save"]},{"cell_type":"code","execution_count":null,"id":"99bf5fa7","metadata":{"id":"99bf5fa7"},"outputs":[],"source":["model.build([(512,35),(512,35)])"]},{"cell_type":"code","execution_count":null,"id":"720ccdcd","metadata":{"id":"720ccdcd"},"outputs":[],"source":["tf.keras.utils.plot_model(model)"]},{"cell_type":"code","execution_count":null,"id":"8fe98351","metadata":{"id":"8fe98351"},"outputs":[],"source":["model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")"]},{"cell_type":"code","execution_count":null,"id":"04599cc4","metadata":{"id":"04599cc4"},"outputs":[],"source":["def predict(ita_text,model):\n","    '''THIS FUNCTION IS USED IN INFERENCE TIME WHICH GIVEN ANY SENTENCE IN ITALIAN OUTPUTS THE ENGLISH SENTENCE AND ALPHA VALUES'''\n","    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n","    seq = tk_inp.texts_to_sequences([ita_text])\n","    # PADDING THE SEQUENCES\n","    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n","    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n","    state = model.layers[0].initialize(1)\n","    # GETTING THE ENCODED OUTPUT\n","    enc_output,state_h,state_c= model.layers[0](seq,state)\n","    # VARIABLE TO STORE PREDICTED SENTENCE\n","    pred = []\n","    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n","    input_state_h = state_h\n","    input_state_c = state_c\n","    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n","    current_vec = tf.ones((1,1))\n","    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n","    alpha_values = []\n","\n","    for i in range(20):\n","        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n","        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c)\n","        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n","        alpha_values.append(alphas)\n","         # UPDATING THE CURRENT VECTOR \n","        current_vec = np.argmax(fc , axis = -1)\n","         # UPDATING THE INPUT STATE\n","        input_state_h = dec_state_h\n","        input_state_c = dec_state_c\n","        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n","        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n","              break\n","    # JOINING THE PREDICTED WORDS\n","    pred_sent = \" \".join(pred)\n","    # CONCATINATING ALL THE ALPHA VALUES\n","    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n","    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n","    return  pred_sent , alpha_values"]},{"cell_type":"code","execution_count":null,"id":"7608b439","metadata":{"id":"7608b439"},"outputs":[],"source":["import nltk.translate.bleu_score as bleu"]},{"cell_type":"code","execution_count":null,"id":"07533096","metadata":{"id":"07533096"},"outputs":[],"source":["BLEU = []\n","index = []\n","np.random.seed(1)\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = predict(str(i.enc_input),model)[0].split()\n","        act = [str(i.dec_output).split()]\n","        b = bleu.sentence_bleu(act,pred)\n","        BLEU.append(b)\n","    except:\n","        index.append(ind)\n","        continue\n","print(\"BELU = \", np.mean(BLEU))"]},{"cell_type":"markdown","id":"593e5f88","metadata":{"id":"593e5f88"},"source":["## PREDICTIONS FOR TEST DATA"]},{"cell_type":"code","execution_count":null,"id":"a8cb69e9","metadata":{"id":"a8cb69e9"},"outputs":[],"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model)[0])\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"]},{"cell_type":"code","execution_count":null,"id":"ebc736a0","metadata":{"id":"ebc736a0"},"outputs":[],"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model)[0])\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"]},{"cell_type":"markdown","id":"21e9b420","metadata":{"id":"21e9b420"},"source":["## Inference Time"]},{"cell_type":"code","execution_count":null,"id":"10fd1167","metadata":{"id":"10fd1167"},"outputs":[],"source":["%%time\n","predict(df_test.enc_input.values[50],model)[0]"]},{"cell_type":"markdown","id":"f3b94190","metadata":{"id":"f3b94190"},"source":["## Beam Search BLUE Score"]},{"cell_type":"code","execution_count":null,"id":"2cbf53c2","metadata":{"id":"2cbf53c2"},"outputs":[],"source":["def beam_search(input,model,k):\n","    seq = tk_inp.texts_to_sequences([input])\n","    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n","\n","    state = model.layers[0].initialize(1)\n","    # GETTING THE ENCODED OUTPUT\n","    enc_output,enc_state_h,enc_state_c = model.layers[0](seq,state)\n","    \n","\n","    input_state_h = enc_state_h\n","    input_state_c = enc_state_c \n","    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n","    for i in range(35):\n","        candidates = []\n","        for sent_pred , prob in k_beams :\n","            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n","                candidates += [[sent_pred , prob]]\n","            else:\n","               \n","                dec_input = model.layers[1].layers[0].layers[0](sent_pred)\n","                dec_output , dec_state_h , dec_state_c   =  model.layers[1].layers[0].layers[2](dec_input ,  initial_state =  [input_state_h , input_state_c])\n","\n","                context_vec , alphas =  model.layers[1].layers[0].layers[1](enc_output,dec_state_h)\n","\n","                # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n","                dense_input =  tf.concat([tf.expand_dims(context_vec,1),tf.expand_dims(dec_state_h,1)],axis=-1)\n","                \n","                # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n","                dense = model.layers[1].layers[0].layers[3](dense_input)\n","\n","                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n","                for w in range(k):\n","                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n","        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n","\n","    all_sent = []\n","    for i,score in k_beams:\n","        sent = \"\"\n","        for j in range(1,35):\n","            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n","            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n","                break\n","        all_sent.append((sent.strip(),score.numpy()))\n","    return all_sent"]},{"cell_type":"code","execution_count":null,"id":"341e0b9d","metadata":{"id":"341e0b9d"},"outputs":[],"source":["# VALIDATION BELU SCORE\n","BLEU_beam = []\n","index = []\n","np.random.seed(1)\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = beam_search(str(i.enc_input),model,3)[0][0].split()\n","        act = [str(i.dec_output).split()]\n","        b =bleu.sentence_bleu(act,pred)\n","        BLEU_beam.append(b)\n","    except:\n","        index.append(ind)\n","        continue\n","\n","print(\"BELU Score = \",np.mean(BLEU_beam))  "]},{"cell_type":"markdown","id":"e9d096dc","metadata":{"id":"e9d096dc"},"source":["### PREDICTIONS ON TEST DATA"]},{"cell_type":"code","execution_count":null,"id":"ec0b6a16","metadata":{"id":"ec0b6a16"},"outputs":[],"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"=\"*50)\n","print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[19])\n","print(\"=\"*50)\n","print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n","bm = (beam_search(df_test.enc_input.values[19],model,3))\n","for i in bm:\n","    print(i)"]},{"cell_type":"code","execution_count":null,"id":"38d2470f","metadata":{"id":"38d2470f"},"outputs":[],"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"=\"*50)\n","print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[50])\n","print(\"=\"*50)\n","print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n","bm = (beam_search(df_test.enc_input.values[50],model,3))\n","for i in bm:\n","    print(i)"]},{"cell_type":"markdown","id":"f62824b0","metadata":{"id":"f62824b0"},"source":["## Model Comparison"]},{"cell_type":"code","execution_count":null,"id":"9b73b37f","metadata":{"id":"9b73b37f"},"outputs":[],"source":["s1 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")\n","s2 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")\n","s3 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")\n","s4 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")\n","s5 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")\n","s6 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/multi_layered_word/besh.h5\")\n","s7 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")\n","s8 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_gernal/best.h5\")\n","s9 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")\n","s10 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","s11 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_general/best.h5\")\n","s12 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_concat/best.h5\")\n"]},{"cell_type":"code","execution_count":null,"id":"e23e3deb","metadata":{"id":"e23e3deb"},"outputs":[],"source":["df_comp = pd.DataFrame()\n","df_comp[\"Model\"] = [\"Encoder Decoder(Char Level)\",\"Encoder Decoder\",\"Encoder Decoder\",\"Encoder Decoder\",\"Bidirectional Encoder Decoder\",\"Multilayered Encoder Decoder\",\"Attention Dot Model\",\"Attention Gernal Model\",\"Attention Concat Model\",\"Monotonic Attention Dot Model\",\"Monotonic Attention Gernal Model\",\"Monotonic Attention Concat Model\"]\n","df_comp[\"Embedding\"] = [\"One Hot Encoding\",\"Trainable Embedding\" , \"Pretrained Word2Vec \" ,\"Fast Text\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\"]\n","df_comp[\"BLEU Score(Greedy Search)\"] = [0.3139,0.4603,0.4453,0.4569,0.4509,0.4527, 0.5055,0.5545,0.5388,0.5469,0.5514,0.5348]\n","df_comp[\"BLEU Score(Beam Search)\"] = [\"-\",\"-\",\"-\",\"-\",0.4561,0.4557,0.5411,0.5324,0.5671,\"-\",\"-\",\"-\"]\n","df_comp[\"Model Size(bytes)\"] = [ s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12]\n","df_comp[\"Model Parameters\"] = [\"616,488\t\",\"26,363,578\" , \"8,158,378\", \"8,158,378\",\"35,018,938\" ,\" 28,464,826\",\"33,353,914\",\"33,419,706\",\"33,485,755\",\"33,353,914\",\"33,419,706\",\"33,485,755\"]\n","df_comp[\"Inference Time(ms)\"] = [143,92.3 , 94.5 , 93.7,250,311,157,164,189,164,179,176]\n","df_comp"]},{"cell_type":"code","execution_count":null,"id":"43621f90","metadata":{"id":"43621f90"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ATTENTION_Concat_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}