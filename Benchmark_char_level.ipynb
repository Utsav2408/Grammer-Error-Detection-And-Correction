{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"Benchmark_char_level.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Zlq3fXZdRepY"},"source":["# ENCODER DECODER MODEL FOR CHARACTER LEVEL EMBEDDING "],"id":"Zlq3fXZdRepY"},{"cell_type":"code","metadata":{"id":"a083ffd7"},"source":["## LOADING THE REQUIRED LIBRARIES\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm \n","import tensorflow as tf\n","from  tensorflow.keras.preprocessing.sequence import pad_sequences\n","from  sklearn.model_selection import train_test_split"],"id":"a083ffd7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7345a1b8"},"source":["## Loading Dataset"],"id":"7345a1b8"},{"cell_type":"code","metadata":{"id":"47TEFxlLGkHY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"47TEFxlLGkHY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f2b8072"},"source":["## LOADING THE PROCESSED DATASET \n","\n","df= pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/cs2/processed_data.csv\")\n","df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n","df[\"dec_output\"] = df.dec_input"],"id":"4f2b8072","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5682004c"},"source":["## Adding start and end token"],"id":"5682004c"},{"cell_type":"code","metadata":{"id":"9cd63a6e"},"source":["## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n","## <start> AS BEGINING TOKEN\n","## <end>  AS END TOKEN\n","\n","df[\"dec_input\"]= \"< \" + df[\"dec_input\"]\n","df[\"dec_output\"] =  df[\"dec_output\"] + \" >\" "],"id":"9cd63a6e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54c6bfe3"},"source":["## SPLITTING ANJ SAMPLITNG AROUD 100K DATAPOINTS\n","\n","\n","---\n","##### THE TOTAL DATASET HAS 500K DATAPOINTS WHICH WILL TAKE MUCH HIGHER TRAINING TIME. THEREFORE I AM SAMPLING ONE-FIFTH OF THE TOTAL DATASET\n"],"id":"54c6bfe3"},{"cell_type":"code","metadata":{"id":"T1NgQRsLxM1r"},"source":["df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))"],"id":"T1NgQRsLxM1r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHbUAaYMWGMU"},"source":["## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n","df_train ,df_val = train_test_split(df_sampled,test_size=0.2,random_state = 3, stratify = df_sampled.y )"],"id":"JHbUAaYMWGMU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"261827ba"},"source":["## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n","df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" >\""],"id":"261827ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wxd7_J_QJf4D"},"source":["## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n","np.random.seed(5)\n","df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]"],"id":"Wxd7_J_QJf4D","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b4ffe0f"},"source":["## Tokenization"],"id":"4b4ffe0f"},{"cell_type":"code","metadata":{"id":"6a248032"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"id":"6a248032","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6f538d5"},"source":["## TOKENIZER FOR ENCODER INPUT\n","tk_inp = Tokenizer(char_level=True )\n","tk_inp.fit_on_texts(df_train.enc_input.apply(str))"],"id":"a6f538d5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e953882"},"source":["# TOKENIZER FOR DECODER INPUT\n","tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' , char_level=True)\n","tk_out.fit_on_texts(df_train.dec_input.apply(str))"],"id":"3e953882","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45d861da"},"source":["## Dataset"],"id":"45d861da"},{"cell_type":"code","metadata":{"id":"0a905874"},"source":["## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n","class Dataset :\n","    def __init__(self, data , tk_inp ,tk_out, max_len):\n","        self.encoder_inp = data[\"enc_input\"].apply(str).values\n","        self.decoder_inp = data[\"dec_input\"].apply(str).values\n","        self.decoder_out = data[\"dec_output\"].apply(str).values\n","        self.tk_inp = tk_inp\n","        self.tk_out = tk_out\n","        self.max_len = max_len\n","      \n","        \n","    def __getitem__(self,i):\n","        # INPUT SEQUENCES\n","        encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n","        # DECODER INPUT SEQUENCES \n","        decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n","        # DECODER INPUT SEQUENCES\n","        decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n","        \n","        # PADDING THE ENCODER INPUT SEQUENCES\n","        encoder_seq = pad_sequences(encoder_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING THE DECODER INPUT SEQUENCES\n","        decoder_inp_seq = pad_sequences(decoder_inp_seq,  padding=\"post\",maxlen = self.max_len)\n","        # PADDING DECODER OUTPUT SEQUENCES\n","        decoder_out_seq = pad_sequences(decoder_out_seq ,  padding=\"post\",maxlen = self.max_len)\n","\n","        ## ONE HOT ENCODING INPUT SEQUENCES\n","        encoder_seq = tf.one_hot(encoder_seq, depth= len(self.tk_inp.word_index)+1 , axis=-1 )\n","        ## ONE HOT ENCODING DECODER INPUT SEQUENCES\n","        decoder_inp_seq = tf.one_hot(decoder_inp_seq, depth= len(self.tk_out.word_index)+1 , axis=-1 )\n","        ## ONE HOT ENCODING DECODER INPUT SEQUENCES\n","        decoder_out_seq = tf.one_hot(decoder_out_seq, depth= len(self.tk_out.word_index)+1, axis=-1 )\n","        return encoder_seq ,  decoder_inp_seq,  decoder_out_seq\n","    \n","    def __len__(self):\n","        # RETURN THE LEN OF INPUT ENDODER\n","        return len(self.encoder_inp)"],"id":"0a905874","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9a336e"},"source":["## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n","\n","class Dataloader(tf.keras.utils.Sequence):\n","    def __init__(self,batch_size,dataset):\n","        # INTIALIZING THE REQUIRED VARIABLES \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.totl_points = self.dataset.encoder_inp.shape[0]\n","        \n","    def __getitem__(self,i):\n","        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n","        start = i * self.batch_size\n","        stop = (i+1)*self.batch_size\n","        \n","        # PLACEHOLDERS FOR BATCHED DATA\n","        batch_enc =[]\n","        batch_dec_input = []\n","        batch_dec_out =[]\n","\n","        for j in range(start,stop): \n","            \n","            a,b,c = self.dataset[j] \n","            batch_enc.append(a[0]) \n","            batch_dec_input.append(b[0])\n","            batch_dec_out.append(c[0]) \n","        \n","        # Conveting list to array   \n","        batch_enc = (np.array(batch_enc)) \n","        batch_dec_input = np.array(batch_dec_input)\n","        batch_dec_out = np.array(batch_dec_out)\n","         ## RETURNING BATCHED DATA IN REQUIRED FORM\n","        return [batch_enc , batch_dec_input],batch_dec_out\n","    \n","    def __len__(self):\n","        # Returning the number of batches\n","        return int(self.totl_points/self.batch_size)"],"id":"2c9a336e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdUY26ULSKjO"},"source":["###### WE ARE TAKING THE MAXIMUM LENGHT EQUAL TO 180 WHICH IS 99 PERCENTILE OF THE DISTRUBUTION OF CHARACTER LENGTH."],"id":"pdUY26ULSKjO"},{"cell_type":"code","metadata":{"id":"86d7d05b"},"source":["# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n","train_dataset = Dataset(df_train,tk_inp,tk_out,180)\n","train_dataloader = Dataloader( batch_size =512, dataset=train_dataset)\n","\n","# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n","val_dataset = Dataset(df_val , tk_inp,tk_out,180)\n","val_dataloader = Dataloader(batch_size= 512 , dataset=val_dataset)"],"id":"86d7d05b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0abfa68"},"source":["## Encoder Decoder Model"],"id":"e0abfa68"},{"cell_type":"code","metadata":{"id":"d06e5d1f"},"source":["## LOADING THE TENSORFLOW LIBRARIES\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model"],"id":"d06e5d1f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLPLNwFyVFwt"},"source":["## DEFINING THE ENCODER LAYER AS A FUNCTION\n","\n","def encoder( lstm_units, enc_input):\n","  ##  LSTM LAYER\n","  enc_lstm , enc_state_h,enc_state_c = layers.LSTM(units= lstm_units,return_sequences=True,return_state=True)(enc_input)\n","  ## RETURNING THE LSTM OUTPUTS AND STATES\n","  return enc_lstm , enc_state_h,enc_state_c\n","\n","## DEFINING THE DECODER LAYER AS A FUNCTION \n","def decoder(lstm_units,enc_states, dec_input):\n","  ## LSTM LAYER\n","  dec_lstm, dec_state_h,dec_state_c = layers.LSTM(units=lstm_units,return_sequences=True,return_state=True)(dec_input,initial_state= enc_states)\n","  ## RETURNING THE LSTM OUTPUTS AND STATES\n","  return dec_lstm, dec_state_h,dec_state_c"],"id":"MLPLNwFyVFwt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuXHo5dzVIOS"},"source":["## DEFINING THE MODEL ARCHITECTURE\n","\n","# INPUT LAYER\n","enc_input = layers.Input(shape=(180,38))\n","# ENCODER DEFINED FORM FUNCTON ABOVE\n","enc_lstm , enc_state_h,enc_state_c = encoder(256, enc_input )\n","\n","# DECODER INPUT LAYER\n","dec_input = layers.Input(shape = (180,40))\n","# DECODER DEFINED FROM ABOVE FUNCTION\n","dec_lstm , dec_state_h,dec_state_c = decoder( 256 , [enc_state_h,enc_state_c] , dec_input)\n","# DENCSE LAYER CONNECTOD TO DECODER OUTPUT\n","dense = layers.Dense(len(tk_out.word_index)+1,activation=\"softmax\")(dec_lstm)\n","\n","# MODEL DEFINING\n","model  = Model(inputs=[enc_input,dec_input],outputs=dense)"],"id":"KuXHo5dzVIOS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpct2PtDjuhm"},"source":["## DEFINING THE CALLBACKS\n","callback =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/Colab Notebooks/cs2/model_save/char_trainable_embedding/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.TensorBoard(log_dir=\"logs_emb/save\", histogram_freq=1),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001),]\n","\n","## STORING THE NUMBER OF STEPS IN ONE EPOCH FOR TRAIN AND VALIDATION DATASET         \n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","# COMPILING THE MODEL\n","model.compile(optimizer=\"adam\",loss='categorical_crossentropy')"],"id":"jpct2PtDjuhm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrYabm0kZbLv"},"source":["history = model.fit(train_dataloader,steps_per_epoch=train_steps,epochs=100,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"],"id":"vrYabm0kZbLv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiQBQ9XRPKkG"},"source":["tf.keras.utils.plot_model(model,show_shapes=True)"],"id":"HiQBQ9XRPKkG","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFG4MRvhLlUi"},"source":["# LOADING THE WEIGHTS FOR BEST MODEL\n","\n","model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")"],"id":"LFG4MRvhLlUi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3967f5b9"},"source":["## THIS FUNCTION IS USED IN THE INFERENCE TIME TO PREDICT THE RESULTS GIVEN THE INPUT TEXT\n","\n","def predict(inp , model):\n","    ##  TAKES INPUT AS TEXT AND THE MODEL\n","\n","    # CONVERT TEXT INPUT TO SEQUENCES \n","    seq = tk_inp.texts_to_sequences([inp])\n","    # PADDING THE SEQUENCE\n","\n","    seq = pad_sequences(seq,maxlen = 180,padding=\"post\")\n","    ## INITIAL STATES FOR ENCODER\n","    state = [tf.zeros(shape=(1,256)),tf.zeros(shape= (1,256))]\n","    # SEQUENCE TO ONE HOT ENCODER VECORS\n","    enc_inp = tf.one_hot(seq , depth= (len(tk_inp.word_index)+1) , axis=-1 )\n","    # PASSING EMBBEDDED SEQUENCES TO LSTM LAYER\n","    enc_output,state_h,state_c= model.layers[2](enc_inp,state)\n","\n","    # PLACE HOLDER FOR PREDECTED WORDS\n","    pred = []\n","    # PLACE HOLDER FOR STATES \n","    input_state = [state_h,state_c]\n","    # CURRENT VECTOR TO BE PASSED TO DECODER \n","    current_vec = tf.convert_to_tensor([[19]],dtype=\"int32\")\n","    for i in range(180):\n","        ## CONVERT THE CURRENT VECTOR SEQUENCE WORD TO EMBEDDINGS\n","        dec_inp = tf.one_hot(current_vec , depth= (len(tk_out.word_index)+1) , axis=-1 )\n","         ## PASSING EMBEDDED VECTOR TO DECODER LSTM LAYER\n","        dec_output,dec_state_h,dec_state_c = model.layers[3](dec_inp , input_state)\n","        # PASSING DECODER OUTPUT TO DENSE LAYER\n","        dense = model.layers[4](dec_output)\n","        # SELECTING INDEX OF MAXIMUM DENSE OUTPUT AS CURRENT VECTOR\n","        current_vec = np.argmax(dense ,axis = -1)\n","        # UPDATING THE INPUT STATES\n","        input_state = [dec_state_h,dec_state_c]\n","        # APPENDING THE ACTUAL TEXT TO \"pred\" VARIABLE\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","         ## IF THE CURRENT VECTOR IS \"<end>\" BREAK THE LOOP\n","        if tk_out.index_word[current_vec[0][0]]==\">\":\n","            break\n","     ## RETURN THE JOINED STRING IN LIST \"pred\"\n","    return \"\".join(pred)"],"id":"3967f5b9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNRtSk0Pk5nz"},"source":["## PREDICTIONS FOR TEST DATASET"],"id":"pNRtSk0Pk5nz"},{"cell_type":"code","metadata":{"id":"xassbnl5Ztnx"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"],"id":"xassbnl5Ztnx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5W5TvCvraLxR"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"],"id":"5W5TvCvraLxR","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VdqLM_nQSY0"},"source":["## INFERENCE TIME"],"id":"7VdqLM_nQSY0"},{"cell_type":"code","metadata":{"id":"mMBUqus1QSJ_"},"source":["%%time\n","predict(df_test.enc_input.values[50],model)"],"id":"mMBUqus1QSJ_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Rrc2qI4xVzR"},"source":["## BELU Score"],"id":"1Rrc2qI4xVzR"},{"cell_type":"code","metadata":{"id":"WCbv5gKGxexg"},"source":["import nltk.translate.bleu_score as bleu"],"id":"WCbv5gKGxexg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIXHF3jw5DEg"},"source":["# VALIDATION BELU SCORE\n","BLEU = []\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = predict(str(i.enc_input),model).split()\n","        act = [str(i.dec_output).split()]\n","        b =bleu.sentence_bleu(act,pred)\n","        BLEU.append(b)\n","    except:\n","        continue"],"id":"aIXHF3jw5DEg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkNCVp6txaes"},"source":["print(\"Triain BELU Score = \",np.mean(BLEU))"],"id":"qkNCVp6txaes","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IE-plzzEPMWT"},"source":["# Model Comparison for Character level Embedding"],"id":"IE-plzzEPMWT"},{"cell_type":"code","metadata":{"id":"2b6788e8"},"source":["s1 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")\n","s2 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")\n","s3 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")\n","s4 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")\n","s5 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")\n","s6 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/multi_layered_word/besh.h5\")\n","s7 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")\n","s8 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_gernal/best.h5\")\n","s9 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")\n","s10 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","s11 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_general/best.h5\")\n","s12 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_concat/best.h5\")\n"],"id":"2b6788e8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw5u7rGsb3Ec"},"source":["df_comp = pd.DataFrame()\n","df_comp[\"Model\"] = [\"Encoder Decoder(Char Level)\",\"Encoder Decoder\",\"Encoder Decoder\",\"Encoder Decoder\",\"Bidirectional Encoder Decoder\",\"Multilayered Encoder Decoder\",\"Attention Dot Model\",\"Attention Gernal Model\",\"Attention Concat Model\",\"Monotonic Attention Dot Model\",\"Monotonic Attention Gernal Model\",\"Monotonic Attention Concat Model\"]\n","df_comp[\"Embedding\"] = [\"One Hot Encoding\",\"Trainable Embedding\" , \"Pretrained Word2Vec \" ,\"Fast Text\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\"]\n","df_comp[\"BLEU Score(Greedy Search)\"] = [0.3139,0.4603,0.4453,0.4569,0.4509,0.4527, 0.5055,0.5545,0.5388,0.5469,0.5514,0.5348]\n","df_comp[\"BLEU Score(Beam Search)\"] = [\"-\",\"-\",\"-\",\"-\",0.4561,0.4557,0.5411,0.5324,0.5671,\"-\",\"-\",\"-\"]\n","df_comp[\"Model Size(bytes)\"] = [ s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12]\n","df_comp[\"Model Parameters\"] = [\"616,488\t\",\"26,363,578\" , \"8,158,378\", \"8,158,378\",\"35,018,938\" ,\" 28,464,826\",\"33,353,914\",\"33,419,706\",\"33,485,755\",\"33,353,914\",\"33,419,706\",\"33,485,755\"]\n","df_comp[\"Inference Time(ms)\"] = [143,92.3 , 94.5 , 93.7,250,311,157,164,189,164,179,176]\n","df_comp"],"id":"Dw5u7rGsb3Ec","execution_count":null,"outputs":[]}]}