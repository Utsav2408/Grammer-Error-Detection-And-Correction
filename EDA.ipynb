{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"EDA.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"fcJo57hxQMUj"},"source":["! pip install jsonlines"],"id":"fcJo57hxQMUj","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfcfb920"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import jsonlines\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter"],"id":"bfcfb920","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ECWjHLEQX29"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"5ECWjHLEQX29","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6d44470c"},"source":["'''THIS CELL READS THE LANG-8 DATASET FILE AND EXTRACTS INCORRECT AND CORRECT SENTENCES ONLY'''\n","\n","f1 = open(\"/content/drive/MyDrive/Colab Notebooks/cs2/lang-8/lang-8-en-1.0/entries.train\") # OPENING THE FILE \n","lines1 = f1.readlines() # READING THE LINES\n","inp1 = [] # LIST FOR STORING INCORRECT SENTENCES\n","tgt1 = [] # LIST FOR STORING\n","\n","for i in lines1: # FOR EACH LINE \n","    lst = i.split(\"\\t\") # WE ARE SPLITTING THE LINE AT \\t\n","    \n","# IF LENGTH OF THE LIST IS GREATER THAN 5 THEN CORRECT SENTTENCE EXISTS OTHERWISE ONLY INCORRECT SENTENCE IS PRESENT \n","    if len(lst)>5  :     #IF LENGTH IS GREATER THAN 5  \n","        inp1.append(lst[-2]) # APPEND SECONG LAST ITEM IN LIST WHICH IS INCORRECT SENTENCE\n","        tgt1.append(lst[-1]) # APPEND LAST ITEM IN THE LIST WHICH IS CORRECT SENTECE."],"id":"6d44470c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2b59ce2"},"source":["'''THIS CELL READS THE SMS_TEXT DATASET FILE AND EXTRACTS INCORRECT AND CORRECT SENTENCES ONLY'''\n","\n","\n","f2 = open(\"/content/drive/MyDrive/Colab Notebooks/cs2/text_sms/release/en2cn-2k.en2nen2cn\",\"r\",encoding=\"UTF-8\") # READING THE FILE\n","\n","lines2 = f2.readlines() # STORING ALL THE LINES IN A VARIABLE\n","inp2 = [] # LIST FOR STORING INCORRECT SENTENCES\n","tgt2 = [] # LIST FOR STORING CORRECT SENTENCES\n","\n","# THE DASET CONTAINS 2000 DATAPOINTS, THEREFORE RUNNING THE LOOP FOR 2000 TIMES\n","for i in range(2000): \n","    inp2.append(lines2[i*3]) #APPEDING FIRST ROW FOR EACH DATAPOINT\n","    tgt2.append(lines2[i*3+1]) # APPENDING SECOND ROW FOR EACH DATAPOINT"],"id":"e2b59ce2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23b028fe"},"source":["## Combining datasets"],"id":"23b028fe"},{"cell_type":"code","metadata":{"id":"7361ab10"},"source":["'''THIS CELL COMBINES BOTH THE DATASETS TOGETHER'''\n","\n","df = pd.DataFrame() # CREATING THE DATAFRAME\n","df[\"input\"] = inp1+inp2 # ADDING BOTH THE LISTS OF INPUTS TO ONE COLUMN\n","df[\"output\"] =  tgt1+tgt2 # ADDING BOTH THE LISTS OF TARGETS TO ONE COLUMN\n","df[\"y\"] = list(\"1\"*len(inp1)) + list(\"2\"*len(inp2))"],"id":"7361ab10","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fede9f01"},"source":["### Data Preprocessing "],"id":"fede9f01"},{"cell_type":"code","metadata":{"id":"6769038d"},"source":["'''THIS FUNCTION REMOVES THE SPACES BETWEEN THE CONTRACTED WORDS AND REMOVING UNNECESSARY SPACES IN THE SENTENCES\n","            ca n't ==> can't \n","            I 'm ===> I'm ...etc\n","'''\n","def remove_spaces(text):\n","    text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n","    text = re.sub(r\" \\,\",\",\",text)\n","    text = re.sub(r\" \\.+\",\".\",text)\n","    text = re.sub(r\" \\!+\",\"!\",text)\n","    text = re.sub(r\" \\?+\",\"?\",text)\n","    text = re.sub(\" n't\",\"n't\",text)\n","    text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n","    \n","    return text\n","\n","\n","'''THIS FUNCTION DECONTRACTS THE CONTRACTED WORDS'''\n","#REF : https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n","\n","def decontract(text):\n","    text = re.sub(r\"won\\'t\", \"will not\", text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    return text\n","\n","\n","'''THIS FUNCTION PREPROCESSES THE TEXT '''\n","def preprocess(text):\n","    text = re.sub(\"\\n\",\"\",text)\n","    text = remove_spaces(text)   # REMOVING UNWANTED SPACES\n","    text = re.sub(r\"\\.+\",\".\",text)\n","    text = re.sub(r\"\\!+\",\"!\",text)\n","    text = decontract(text)    # DECONTRACTION\n","    text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n","    text = text.lower()\n","    return text"],"id":"6769038d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9143e2ed"},"source":["'''HERE WE ARE APPLYIN PREPROCESS FUNCTION TO INPUT AND OUTPUT SENTENCES'''\n","\n","df[\"processed_input\"] = df.input.apply(preprocess) # APPLYING PREPROCESS FUNCTION TO INPUT \n","df[\"processed_output\"] = df.output.apply(preprocess) # APPLYING PREPROCESS FUNCION TO OUTPUT"],"id":"9143e2ed","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7a9bde99"},"source":["df =df.drop([\"input\",\"output\"],axis=1)"],"id":"7a9bde99","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1e3d315d"},"source":["## Exploartory Data Analysis"],"id":"1e3d315d"},{"cell_type":"markdown","metadata":{"id":"0f7fcfa0"},"source":["### Removing null values"],"id":"0f7fcfa0"},{"cell_type":"code","metadata":{"id":"543d5228"},"source":["'''THIS CELL REMOVES ROWS WITH NULL VALUES IN INPUT AND OUTPUT TEXT'''\n","\n","df = df[df.processed_input.notnull()]\n","df = df[df.processed_output.notnull()]"],"id":"543d5228","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ab9b1f1d"},"source":["#### Observations:\n","* By removing the null values we have reduced the number of rows from 511163 to 511123"],"id":"ab9b1f1d"},{"cell_type":"markdown","metadata":{"id":"fb6293f3"},"source":["### Removing Duplicates"],"id":"fb6293f3"},{"cell_type":"code","metadata":{"id":"b66b6674"},"source":["'''DROPPING THE DUPLICATES'''\n","\n","df = df.drop_duplicates()"],"id":"b66b6674","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0454fd2a"},"source":["#### OBSERVATION:\n","* By dropping duplicates we can reduce the number of datapoints from 511123 to 505890"],"id":"0454fd2a"},{"cell_type":"markdown","metadata":{"id":"115ec2a3"},"source":["# Wrong words and Corrected words analysis"],"id":"115ec2a3"},{"cell_type":"markdown","metadata":{"id":"695f410e"},"source":["#####  Wrong words : words present in input but not present in target sentences \n","##### Corrected words : words present in target sentences but not present in input sentences."],"id":"695f410e"},{"cell_type":"code","metadata":{"scrolled":false,"id":"a389e6ed"},"source":["'''THIS FUNCTION TAKES IN THE DATAFRAME AND OUPUTS THE CORPUS FOR WRONG WORDS AND CORRECTED WORDS'''\n","\n","def wrong_words(df):\n","    # CONVERTING THE SENTENCES OF INPUT TO WORDS AND CONVERTING TO SETS\n","    inp = df.processed_input.apply(lambda x: set(str(x).split()))\n","    \n","    # CONVERTING THE SENTENCES OF OUTPUT TO WORDS AND CONVERTING TO SETS\n","    out = df.processed_output.apply(lambda x: set(str(x).split()))\n","    \n","    # GETTIN THE SET OF WRONG WORDS \n","    df_ww = (inp-out).apply(list)\n","    \n","    # GETTING THE ST OF CORRECTED SENTENCES\n","    df_corrected = (out-inp).apply(list)\n","    \n","    # JOINING ALL WORDS AND FORIMING A LONG TEXT \n","    ww_corpus = \"\"\n","    for i in df_ww.values:\n","        \n","        if len(i)>0:\n","            ww_corpus+= \" \"+ \" \".join(i) \n","            \n","    # JOINING ALL WORDS AND FORIMING A LONG TEXT \n","    corr_corpus =\"\"       \n","    for i in df_corrected.values:\n","        if len(i)>0:\n","            corr_corpus += \" \"+ \" \".join(i)    \n","        \n","    # RETURNING THE WRONG WORDS TEXT AND CORRECTED WORDS TEXT          \n","    return ww_corpus,corr_corpus"],"id":"a389e6ed","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"72c59a86"},"source":["'''THIS CELL FORMS THE WORD CLOUD FOR WRONG WORDS AND CORRECTED WORDS'''\n","\n","# GETTING THE TEXT FOR WRONG WORDS AND CORRECTED WORDS\n","ww_corpus,corr_corpus = wrong_words(df)\n","\n","# FORMING WORD CLOUD FOR WROND WORDS\n","wordcloud_ww = WordCloud(width = 800, height = 800,\n","                 background_color ='black',\n","                 min_font_size = 10).generate(ww_corpus)\n","\n","# FORMING WORD CLOUD FOR CORRECTED WORDS\n","wordcloud_corr = WordCloud(width = 800, height = 800,\n","                 background_color ='black',\n","                 min_font_size = 10).generate(corr_corpus)\n","\n"],"id":"72c59a86","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6839a312"},"source":["## POS Tagging"],"id":"6839a312"},{"cell_type":"markdown","metadata":{"id":"937df209"},"source":["# Input"],"id":"937df209"},{"cell_type":"code","metadata":{"id":"3039a6d6"},"source":["def pos(text):\n","    pos = nltk.pos_tag(word_tokenize(text))\n","    pos = [j for i ,j in pos]\n","    return pos\n","\n","tags_input = df.processed_input.apply(lambda x: pos(x))\n","tags_input = [ j for i in tags_input for j in i]\n","counter_input = Counter(tags_input) \n","inp_10 = list(zip(*counter_input.most_common(10)))"],"id":"3039a6d6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba8e4299"},"source":["# Output"],"id":"ba8e4299"},{"cell_type":"code","metadata":{"id":"7463120e"},"source":["def pos(text):\n","    pos = nltk.pos_tag(word_tokenize(text))\n","    pos = [j for i ,j in pos]\n","    return pos\n","\n","tags_output = df.processed_output.apply(lambda x: pos(x))\n","tags_output = [ j for i in tags_output for j in i]\n","counter_output = Counter(tags_output) \n","out_10 = list(zip(*counter_output.most_common(10)))"],"id":"7463120e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cd3ff3f"},"source":["df[[\"processed_input\",\"processed_output\",\"y\"]].to_csv(\"/content/drive/MyDrive/Colab Notebooks/cs2/processed_data.csv\",index=False)"],"id":"0cd3ff3f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e98f0e77"},"source":[""],"id":"e98f0e77","execution_count":null,"outputs":[]}]}