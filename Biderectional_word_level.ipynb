{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Biderectional_word_level.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3DMBfFOYRnzv"},"source":["# ENCODER DECODER MODEL FOR WORD LEVEL EMBEDDING"],"id":"3DMBfFOYRnzv"},{"cell_type":"code","metadata":{"id":"a083ffd7"},"source":["## LOADING THE REQUIRED LIBRARIES\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm \n","import tensorflow as tf\n","from  tensorflow.keras.preprocessing.sequence import pad_sequences\n","from  sklearn.model_selection import train_test_split\n","from tqdm import tqdm"],"id":"a083ffd7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7345a1b8"},"source":["## Loading Dataset"],"id":"7345a1b8"},{"cell_type":"code","metadata":{"id":"47TEFxlLGkHY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"47TEFxlLGkHY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f2b8072"},"source":["## LOADING THE PROCESSED DATASET\n","\n","df= pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/cs2/processed_data.csv\")\n","df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n","df[\"dec_output\"] = df.dec_input"],"id":"4f2b8072","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5682004c"},"source":["## Adding start and end token"],"id":"5682004c"},{"cell_type":"code","metadata":{"id":"9cd63a6e"},"source":["## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n","## <start> AS BEGINING TOKEN\n","## <end>  AS END TOKEN\n","\n","df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n","df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" "],"id":"9cd63a6e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54c6bfe3"},"source":["## Splitting And Sampling around 100k datapoints\n","\n","---\n","##### THE TOTAL DATASET HAS 500K DATAPOINTS WHICH WILL TAKE MUCH HIGHER TRAINING TIME. THEREFORE I AM SAMPLING ONE-FIFTH OF THE TOTAL DATASET\n","\n"],"id":"54c6bfe3"},{"cell_type":"code","metadata":{"id":"T1NgQRsLxM1r"},"source":["df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))"],"id":"T1NgQRsLxM1r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHbUAaYMWGMU"},"source":["## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n","\n","df_train ,df_val = train_test_split(df_sampled,test_size=0.2,random_state = 3, stratify = df_sampled.y )"],"id":"JHbUAaYMWGMU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"261827ba"},"source":["## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n","\n","df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\""],"id":"261827ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_0LH-ZVgHPH"},"source":["## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n","np.random.seed(5) \n","df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]"],"id":"y_0LH-ZVgHPH","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b4ffe0f"},"source":["## Tokenization"],"id":"4b4ffe0f"},{"cell_type":"code","metadata":{"id":"6a248032"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"id":"6a248032","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6f538d5"},"source":["## TOKENIZER FOR ENCODER INPUT\n","tk_inp = Tokenizer()\n","tk_inp.fit_on_texts(df_train.enc_input.apply(str))"],"id":"a6f538d5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e953882"},"source":["# TOKENIZER FOR DECODER INPUT\n","tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n","tk_out.fit_on_texts(df_train.dec_input.apply(str))"],"id":"3e953882","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45d861da"},"source":["## Dataset Loader"],"id":"45d861da"},{"cell_type":"code","metadata":{"id":"0a905874"},"source":["## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n","class Dataset :\n","    def __init__(self, data , tk_inp ,tk_out, max_len):\n","        self.encoder_inp = data[\"enc_input\"].apply(str).values\n","        self.decoder_inp = data[\"dec_input\"].apply(str).values\n","        self.decoder_out = data[\"dec_output\"].apply(str).values\n","        self.tk_inp = tk_inp\n","        self.tk_out = tk_out\n","        self.max_len = max_len\n","        \n","    def __getitem__(self,i):\n","        # INPUT SEQUENCES\n","        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n","        # DECODER INPUT SEQUENCES \n","        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n","        # DECODER INPUT SEQUENCES\n","        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n","        \n","        # PADDING THE ENCODER INPUT SEQUENCES\n","        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING THE DECODER INPUT SEQUENCES\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n","        # PADDING DECODER OUTPUT SEQUENCES\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n","        ##  RETURNING THE ENCODER INPUT , DECODER INPUT , AND DECODER OUTPUT\n","        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n","    \n","    def __len__(self):\n","        # RETURN THE LEN OF INPUT ENDODER\n","        return len(self.encoder_inp)"],"id":"0a905874","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9a336e"},"source":["## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n","\n","class Dataloader(tf.keras.utils.Sequence):\n","    def __init__(self,batch_size,dataset):\n","        # INTIALIZING THE REQUIRED VARIABLES \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.totl_points = self.dataset.encoder_inp.shape[0]\n","        \n","    def __getitem__(self,i):\n","        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n","        start = i * self.batch_size\n","        stop = (i+1)*self.batch_size\n","        \n","        # PLACEHOLDERS FOR BATCHED DATA\n","        batch_enc =[]\n","        batch_dec_input = []\n","        batch_dec_out =[]\n","\n","        for j in range(start,stop): \n","            \n","            a,b,c = self.dataset[j] \n","            batch_enc.append(a[0]) \n","            batch_dec_input.append(b[0])\n","            batch_dec_out.append(c[0]) \n","        \n","        # Conveting list to array   \n","        batch_enc = (np.array(batch_enc)) \n","        batch_dec_input = np.array(batch_dec_input)\n","        batch_dec_out = np.array(batch_dec_out)\n","        \n","        return [batch_enc , batch_dec_input],batch_dec_out\n","    \n","    def __len__(self):\n","        # Returning the number of batches\n","        return int(self.totl_points/self.batch_size)"],"id":"2c9a336e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfEi9fzDR3FD"},"source":["###### WE ARE TAKING THE MAXIMUM LENGHT EQUAL TO 35 WHICH IS 99 PERCENTILE OF THE WORD LENGTH DISTRUBUTION"],"id":"QfEi9fzDR3FD"},{"cell_type":"code","metadata":{"id":"86d7d05b"},"source":["# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n","train_dataset = Dataset(df_train,tk_inp,tk_out,35)\n","train_dataloader = Dataloader( batch_size = 512, dataset=train_dataset)"],"id":"86d7d05b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b3bbfe7"},"source":["# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n","val_dataset = Dataset(df_val , tk_inp,tk_out,35)\n","val_dataloader = Dataloader(batch_size=512 , dataset=val_dataset)"],"id":"5b3bbfe7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0abfa68"},"source":["## Encoder Decoder Model"],"id":"e0abfa68"},{"cell_type":"code","metadata":{"id":"d06e5d1f"},"source":["## LOADING THE TENSORFLOW LIBRARIES\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model"],"id":"d06e5d1f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhQkUiBWymJI"},"source":["## ENDOCER CLASS\n","class Encoder(tf.keras.layers.Layer):\n","    '''\n","    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n","    '''\n","    def __init__(self , vocab_size , embedding_dim , enc_units , input_len):\n","        super().__init__()\n","        \n","        # STATING ALL THE VARIABLES\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.input_len = input_len\n","        self.enc_units = enc_units\n","        \n","        # INITALIZING EMBEDDING LAYER\n","        self.embedding = layers.Embedding(input_dim= self.vocab_size,\n","                                         output_dim = self.embedding_dim,\n","                                         mask_zero = True,\n","                                          input_length = self.input_len\n","                                         )\n","        # INTIALIZING LSTM LAYER\n","        self.lstm_bi = layers.Bidirectional(layers.LSTM(units= self.enc_units,return_state = True,return_sequences=True ))\n","        self.concat1 = layers.Concatenate()\n","        self.concat2 = layers.Concatenate()\n","    def call(self,input):\n","        '''\n","          This function takes a sequence input and the initial states of the encoder.\n","          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n","          returns -- encoder_output, last time step's hidden and cell state\n","        '''\n","        # CONVERTING INPUT TO EMBEDDED VECTORS\n","        emb = self.embedding(input)\n","        # PASSING THROUGH LSTM LAYER\n","        enc_output , state_h1 , state_c1 ,state_h2 , state_c2 = self.lstm_bi(emb)\n","        state_h = self.concat1([state_h1,state_h2])\n","        state_c  = self.concat2(([state_c1,state_c2]))\n","\n","        return enc_output ,state_h ,state_c \n","\n","## DECODER CLASS\n","class Decoder(tf.keras.layers.Layer):\n","    def __init__(self,vocab_size , embedding_dim, dec_unit,input_len ):\n","        super().__init__()\n","        # INITALIZING ALL THE VARIABLES \n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.input_len = input_len\n","        self.dec_unit =dec_unit\n","        \n","        \n","    def build(self,input_shape):\n","        \n","        # INITALIZING EMBEDDING AND LSTM LAYER\n","        self.embedding = layers.Embedding(input_dim = self.vocab_size,\n","                                          output_dim = self.embedding_dim,\n","                                         mask_zero=True,\n","                                         input_length = self.input_len)\n","        self.lstm = layers.LSTM(units = self.dec_unit,\n","                               return_sequences=True,\n","                               return_state=True)\n","        \n","    def call(self,input, state):\n","        # FORMING THE EMBEDDED VECTORS\n","        emb = self.embedding(input)\n","        \n","        # LSTM OUTPUT\n","        dec_out,state_h,state_c = self.lstm(emb,initial_state = state)\n","        \n","        return dec_out,state_h,state_c\n","  "],"id":"KhQkUiBWymJI","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDb0xR5CymJO"},"source":["## DEFINING THE ARCHITECTURE\n","\n","# INPUT LAYER\n","enc_input = layers.Input(shape=(35))\n","# ENCODER \n","enc_output,state_h,state_c =   Encoder(vocab_size= len(tk_inp.word_index)+1  , embedding_dim= 300 ,\n","                               enc_units=256 ,input_len=35)(enc_input)\n","## STORING ENCOER STATES IN A VARIABLE\n","enc_state = [state_h,state_c] \n","\n","# INPUT LAYER FOR DECODER\n","dec_input = layers.Input(shape = (35))\n","## DECODER LAYER\n","dec_output,_,_ = Decoder(vocab_size = len(tk_out.word_index)+1  , embedding_dim = 300,\n","                               dec_unit=512,input_len=35)(dec_input,enc_state)\n","\n","# DENSE LAYER\n","dense = layers.Dense(len(tk_out.word_index)+1,activation=\"softmax\")(dec_output)\n","\n","# MODEL DEFINING\n","model  = Model(inputs=[enc_input,dec_input],outputs=dense)                           "],"id":"uDb0xR5CymJO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfwFU6kxymJP"},"source":["## DEFINING THE CALLBACKS\n","\n","callback =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/Colab Notebooks/cs2/model_save/bidirectional_train_emb/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n","           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001),\n","            tf.keras.callbacks.TensorBoard(\"/content/drive/MyDrive/Colab Notebooks/cs2/model_save/bidirectional_train_emb/logs/save\",histogram_freq=1)\n","]\n","## TRAINNG AND VALIDATION STEPS FOR ONE EPOCH\n","train_steps = train_dataloader.__len__()\n","val_steps  = val_dataloader.__len__()\n","\n","# COMPILING THE MODEL\n","model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"],"id":"zfwFU6kxymJP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Db9oJ3IrymJP"},"source":["history = model.fit(train_dataloader,steps_per_epoch=train_steps,epochs=50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"],"id":"Db9oJ3IrymJP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTWL1JS82C5t"},"source":["### LOADING THE BEST WEIGHTS\n","model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")"],"id":"jTWL1JS82C5t","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3967f5b9"},"source":["## THIS FUNCTION IS USED IN THE INFERENCE TIME TO PREDICT THE RESULTS GIVEN THE INPUT TEXT\n","\n","def predict(ita_text,model):\n","    \n","    # forming integer sequences\n","    seq = tk_inp.texts_to_sequences([ita_text])\n","    # padding the sequences\n","    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n","    \n","    # generating the output from encoder\n","    enc_output,state_h,state_c= model.layers[2](seq)\n","    \n","    # placeholder for predicted output\n","    pred = []\n","    \n","    input_state = [state_h,state_c]\n","    # initailizing the vector for inputing to decoder\n","    current_vec = tf.ones((1,1))\n","    \n","    for i in range(20): # for each word in the input\n","        # passing each word through decoder layer\n","        dec_output,dec_state_h,dec_state_c = model.layers[3](current_vec , input_state)\n","        # passing decoder output through dense  layer\n","        dense = model.layers[4](dec_output)\n","        # taking argmax and getting the word index and updating the current vector\n","        current_vec = np.argmax(dense ,axis = -1)\n","        # updating the decoder states\n","        input_state = [dec_state_h,dec_state_c]\n","        # getting the actual word from the vocab\n","        pred.append(tk_out.index_word[current_vec[0][0]])\n","        \n","        # if the actual word is <end> break the loop\n","        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n","            break\n","        \n","    return \" \".join(pred)"],"id":"3967f5b9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zexRbu3Uugc4"},"source":["## IMPORTING THE BLUE SCORE\n","import nltk.translate.bleu_score as bleu"],"id":"zexRbu3Uugc4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1-Q51inyRfi"},"source":["# BELU SCORE\n","BLEU = []\n","np.random.seed(1)\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","    try:\n","        pred = predict(str(i.enc_input),model).split()\n","        act = [str(i.dec_output).split()]\n","        b = bleu.sentence_bleu(act,pred)\n","        BLEU.append(b)\n","    except:\n","      continue\n","print(\"BELU = \", np.mean(BLEU))"],"id":"M1-Q51inyRfi","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jV16qSznKzi"},"source":["## PREDICTIONS ON TEST DATASET"],"id":"3jV16qSznKzi"},{"cell_type":"code","metadata":{"id":"vbeFQ9MBeFrB"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"],"id":"vbeFQ9MBeFrB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvsZIIoweFrC"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model))\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"],"id":"pvsZIIoweFrC","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYIk_6_yoyKp"},"source":["## INFERENCE TIME"],"id":"IYIk_6_yoyKp"},{"cell_type":"code","metadata":{"id":"H91WF1I6oxnf"},"source":["%%time\n","predict(df_test.enc_input.values[50],model)"],"id":"H91WF1I6oxnf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRXQJ7vrJzS0"},"source":["def beam_search(input,model,k):\n","    seq = tk_inp.texts_to_sequences([input])\n","    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n","\n","    \n","    enc_gru ,enc_state_h,enc_state_c   = model.layers[2](seq)\n","\n","    input_state = [enc_state_h,enc_state_c ]\n","    \n","    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n","    for i in range(35):\n","        candidates = []\n","        for sent_pred , prob in k_beams :\n","            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n","\n","                candidates += [[sent_pred , prob]]\n","            else:\n","                \n","                dec_gru , dec_state_h ,dec_state_c = model.layers[3](sent_pred , input_state)\n","                dense = model.layers[4](tf.expand_dims(dec_state_h,axis=0))\n","                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n","                for w in range(k):\n","                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n","        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n","\n","    all_sent = []\n","    for i,score in k_beams:\n","        sent = \"\"\n","        for j in range(1,35):\n","            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n","            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n","                break\n","        all_sent.append((sent.strip(),score.numpy()))\n","    return all_sent"],"id":"aRXQJ7vrJzS0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SuXzmrQyRca"},"source":["# VALIDATION BELU SCORE\n","BLEU_beam = []\n","np.random.seed(1)\n","test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n","for ind,i in tqdm(test_data.iterrows(),position=0):\n","\n","    try:\n","        pred = beam_search(str(i.enc_input),model,3)[0][0].split()\n","        act = [str(i.dec_output).split()]\n","        b =bleu.sentence_bleu(act,pred)\n","        BLEU_beam.append(b)\n","    except:\n","          continue\n","\n","print(\"BELU Score = \",np.mean(BLEU_beam))  "],"id":"2SuXzmrQyRca","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sg9CUEYNPsUO"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n","print(\"=\"*50)\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])\n","print(\"=\"*50)\n","print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n","bm = (beam_search(df_test.enc_input.values[19],model,3))\n","for i in bm:\n","    print(i)"],"id":"sg9CUEYNPsUO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZXCLw8doNVw"},"source":["print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n","print(\"=\"*50)\n","print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])\n","print(\"=\"*50)\n","print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n","bm = (beam_search(df_test.enc_input.values[50],model,3))\n","for i in bm:\n","    print(i)"],"id":"9ZXCLw8doNVw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXVCnjvQLZFh"},"source":["## Model Comparison"],"id":"cXVCnjvQLZFh"},{"cell_type":"code","metadata":{"id":"2b6788e8"},"source":["s1 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")\n","s2 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")\n","s3 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")\n","s4 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")\n","s5 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")\n","s6 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/multi_layered_word/besh.h5\")\n","s7 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")\n","s8 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_gernal/best.h5\")\n","s9 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")\n","s10 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n","s11 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_general/best.h5\")\n","s12 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_concat/best.h5\")\n"],"id":"2b6788e8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw5u7rGsb3Ec"},"source":["df_comp = pd.DataFrame()\n","df_comp[\"Model\"] = [\"Encoder Decoder(Char Level)\",\"Encoder Decoder\",\"Encoder Decoder\",\"Encoder Decoder\",\"Bidirectional Encoder Decoder\",\"Multilayered Encoder Decoder\",\"Attention Dot Model\",\"Attention Gernal Model\",\"Attention Concat Model\",\"Monotonic Attention Dot Model\",\"Monotonic Attention Gernal Model\",\"Monotonic Attention Concat Model\"]\n","df_comp[\"Embedding\"] = [\"One Hot Encoding\",\"Trainable Embedding\" , \"Pretrained Word2Vec \" ,\"Fast Text\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\"]\n","df_comp[\"BLEU Score(Greedy Search)\"] = [0.3139,0.4603,0.4453,0.4569,0.4509,0.4527, 0.5055,0.5545,0.5388,0.5469,0.5514,0.5348]\n","df_comp[\"BLEU Score(Beam Search)\"] = [\"-\",\"-\",\"-\",\"-\",0.4561,0.4557,0.5411,0.5324,0.5671,\"-\",\"-\",\"-\"]\n","df_comp[\"Model Size(bytes)\"] = [ s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12]\n","df_comp[\"Model Parameters\"] = [\"616,488\t\",\"26,363,578\" , \"8,158,378\", \"8,158,378\",\"35,018,938\" ,\" 28,464,826\",\"33,353,914\",\"33,419,706\",\"33,485,755\",\"33,353,914\",\"33,419,706\",\"33,485,755\"]\n","df_comp[\"Inference Time(ms)\"] = [143,92.3 , 94.5 , 93.7,250,311,157,164,189,164,179,176]\n","df_comp"],"id":"Dw5u7rGsb3Ec","execution_count":null,"outputs":[]}]}